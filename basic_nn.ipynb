{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simaliar to the first attempt but with cleaner code, more flexible layer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "print(\"start!\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    input_layer_size: int\n",
    "    output_layer_size: int\n",
    "    hidden_layer_num : int\n",
    "    hidden_layer_size : int\n",
    "\n",
    "    dataset_size : int\n",
    "    learning_rate :float\n",
    "    batch_size : int\n",
    "    epochs : int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer(layer_size, dtype=np.float32, **kwargs):\n",
    "    layer = np.zeros(layer_size, dtype=dtype)\n",
    "    return(layer)\n",
    "\n",
    "def create_layer_weights(prev_ly_size, curr_ly_size, init=True):\n",
    "    if init:\n",
    "        ly_weights = np.random.rand(prev_ly_size, curr_ly_size)\n",
    "    else:\n",
    "        ly_weights = np.zeros(shape=(prev_ly_size, curr_ly_size))\n",
    "    return(ly_weights)\n",
    "\n",
    "def create_biases(ly_size, init=True):\n",
    "    if init:\n",
    "        biases = np.random.rand(ly_size)\n",
    "    else:\n",
    "        biases = np.zeros(ly_size)\n",
    "    return(biases)\n",
    "\n",
    "\n",
    "def label_to_output(int) -> np.ndarray:\n",
    "    outlayer = np.zeros(shape=(10), dtype=np.float32)\n",
    "    outlayer[int] = 1\n",
    "    return(outlayer)\n",
    "\n",
    "def label_vec_to_output(inputs: np.ndarray) -> np.ndarray:\n",
    "    # Create an identity matrix of size 10 and index into it\n",
    "    return np.eye(10, dtype=np.int8)[inputs]\n",
    "\n",
    "class n_network:\n",
    "    def __init__(self, layers, biases, weights):\n",
    "        self.layers = layers\n",
    "        self.biases = biases\n",
    "        self.weights = weights\n",
    "\n",
    "def create_network(in_ly_size, out_ly_size, hidden_ly_num, hidden_ly_size):\n",
    "    in_layer = create_layer(in_ly_size)\n",
    "    hidden_lys = []\n",
    "    for index in range(hidden_ly_num):\n",
    "        hidden_lys.append(create_layer(hidden_ly_size))\n",
    "    out_layer = create_layer(out_ly_size)\n",
    "    network_lys = [in_layer, *hidden_lys, out_layer]\n",
    "\n",
    "\n",
    "    biases =[]\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        biases.append(create_biases(np.size(network_lys[i]), init=False))\n",
    "\n",
    "    weights = []\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        weights.append( create_layer_weights(np.size(network_lys[i-1]), np.size(network_lys[i]), init=True))\n",
    "\n",
    "    network = n_network(network_lys, biases, weights)\n",
    "    return(network)\n",
    "\n",
    "\n",
    "def sigx(number):\n",
    "    #sigmond function\n",
    "    #todo: make a nativly vectorized verison of this \n",
    "    try:\n",
    "        sig_x = ( 1 / (1+ math.exp(-number)))\n",
    "        #this does not woprk for large negitive numbers \n",
    "        # due to rounding errors leading to devide by zero so we'll add error handling\n",
    "    except OverflowError:\n",
    "        sig_x = 0\n",
    "    return(sig_x)\n",
    "\n",
    "def forward(input_vals:np.ndarray, net:n_network):\n",
    "    #first layer\n",
    "    input_vals = np.array(input_vals)\n",
    "    net.layers[0] = input_vals\n",
    "\n",
    "    for index in range(1, len(net.layers), 1):\n",
    "        net.layers[index] = np.dot(net.layers[(index-1)], net.weights[index-1]) + net.biases[index-1]\n",
    "        \n",
    "        #activation function\n",
    "        sigVec = np.vectorize(sigx)\n",
    "        net.layers[index] = sigVec(net.layers[index])\n",
    "    return(net)\n",
    "\n",
    "def node_delta(expected:np.array, actual:np.array):\n",
    "    delta = expected - actual\n",
    "    return(delta)\n",
    "\n",
    "def deep_copy_mat_list (list_to_copy, propigate_cell_vals=True):\n",
    "    #creates a copy of a list containing numpy arrays of varible size, ethier as empty arrays or with the same values\n",
    "    \n",
    "    new_list = []\n",
    "    if propigate_cell_vals:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(items)\n",
    "    else:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(np.zeros_like(items))       \n",
    "\n",
    "    return(new_list)\n",
    "\n",
    "\n",
    "\n",
    "def loss(network_output_ly, expected_ly):\n",
    "    #loss =  mean of all output neurons - the expected, squared\n",
    "    loss_vec = (network_output_ly - expected_ly)**2\n",
    "    loss_sclr = np.sum((network_output_ly - expected_ly)**2) / len(network_output_ly)\n",
    "    return(loss_sclr, loss_vec)\n",
    "\n",
    "def backprop_single(net: n_network, expected_out_ly):\n",
    "    #does backprop for a single training example, duhh\n",
    "    #add learning rate??\n",
    "    \n",
    "    expected_out_ly = np.array(expected_out_ly)\n",
    "    w_delta = deep_copy_mat_list(net.weights, propigate_cell_vals=False)\n",
    "    b_delta = deep_copy_mat_list(net.biases, propigate_cell_vals=False)\n",
    "    a_delta = deep_copy_mat_list(net.layers, propigate_cell_vals=False)\n",
    "\n",
    "    first = True\n",
    "    for l in range(len(net.layers)-1, 0, -1):\n",
    "        #print(\"at l of:\", l)\n",
    "\n",
    "        if first:\n",
    "            #first(last) layer\n",
    "            first = False\n",
    "            a_delta_dir = expected_out_ly - net.layers[l]  \n",
    "            w_delta[l-1] = np.outer(net.layers[l-1], a_delta_dir)   \n",
    "            a_delta[l] = a_delta_dir\n",
    "        else:\n",
    "            a_delta_dir = a_delta[l] - net.layers[l]\n",
    "            w_delta[l-1] = np.outer(net.layers[l-1], a_delta_dir)   \n",
    "\n",
    "        a_delta[l-1] = np.dot(net.weights[l-1], net.layers[l])\n",
    "        a_delta[l-1] = a_delta[l-1] / a_delta[l-1].size\n",
    "    return(w_delta, b_delta)\n",
    "\n",
    "def create_batch(cfg:Config, batch_position, image_file, label_file):\n",
    "   \n",
    "    image_buffer = image_file.read(cfg.input_layer_size * cfg.batch_size)\n",
    "    input_batch = np.frombuffer(image_buffer, dtype=np.uint8).astype(np.float32)\n",
    "    input_batch = input_batch.reshape(cfg.batch_size, cfg.input_layer_size)\n",
    "    \n",
    "    label_buffer = label_file.read(cfg.batch_size)\n",
    "    label_batch = np.frombuffer(label_buffer, dtype=np.uint8).astype(np.int64)\n",
    "    Label_batch = label_vec_to_output(label_batch)\n",
    "\n",
    "    return(input_batch, Label_batch)\n",
    "    \n",
    "\n",
    "def batch_learning(input_batch:np.ndarray, desired_output_batch:np.ndarray, net:n_network, cfg:Config):\n",
    "    cfg.batch_size\n",
    "    w_del_batch = []\n",
    "    b_del_batch = []\n",
    "    for layers in net.weights:\n",
    "        w_del_batch.append(np.zeros((cfg.batch_size,) + layers.shape, dtype=np.float32)  )\n",
    "\n",
    "    for i in range(cfg.batch_size):\n",
    "        net = forward(input_batch[i], net)\n",
    "        w_del_batch_tmp, b_del_batch_tmp = backprop_single(net, desired_output_batch[i])\n",
    "\n",
    "        for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "        w_del_batch[layer_num] = w_del_batch_tmp[layer_num]\n",
    "    \n",
    "    print(type(w_del_batch))\n",
    "    print(len(w_del_batch))\n",
    "    for items in w_del_batch:\n",
    "        print(type(items))\n",
    "        print(np.shape(items))\n",
    "        #for stuff in items:\n",
    "         #   print(type(stuff))\n",
    "        #    print(np.shape(stuff))\n",
    "    #for index in range(cfg.hidden_layer_num + 1):\n",
    "    #for index in range(4):\n",
    "     #   useful = w_del_batch[:]\n",
    "      #  print(\"the hopefully isolated array: \\n\")\n",
    "       # print(type(useful))\n",
    "        #print(len(useful))\n",
    "        #print(useful)\n",
    "    return(gwfioen)\n",
    "\n",
    "\n",
    "    w_delta_mean = []\n",
    "\n",
    "\n",
    "    b_delta_mean = []\n",
    "    descent_step = w_delta_mean, b_delta_mean\n",
    "    #return(descent_step)\n",
    "\n",
    "\n",
    "def train(images_pth, labels_pth, cfg:Config):\n",
    "\n",
    "    net = create_network(cfg.input_layer_size, cfg.output_layer_size, cfg.hidden_layer_num, cfg.hidden_layer_size)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(\"epoch \", int(epoch), \"out of \", int(cfg.epochs))\n",
    "        f = open(images_pth, 'rb')\n",
    "        f.read(16)\n",
    "        l = open(labels_pth,'rb')\n",
    "        l.read(8)\n",
    "        for index in range(0, cfg.dataset_size, cfg.batch_size):\n",
    "            print(\"index position: \", int(index))\n",
    "            input_ly, label_ly = create_batch(cfg, index, f, l)\n",
    "            batch_learning(input_ly, label_ly, net,cfg)\n",
    "            #W_delta, B_delta = batch_learning(input_ly, label_ly, net,cfg)\n",
    "            #W_delta = W_delta * cfg.learning_rate\n",
    "            #B_delta = B_delta * cfg.learning_rate\n",
    "\n",
    "            #net.weights = net.weights + W_delta\n",
    "            #net.biases = net.biases + B_delta\n",
    "\n",
    "\n",
    "    #do batches here:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 out of  2\n",
      "index position:  0\n",
      "<class 'list'>\n",
      "4\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 784, 17)\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 17, 17)\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 17, 17)\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 17, 10)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gwfioen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 23\u001b[0m\n\u001b[1;32m      9\u001b[0m labels_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST/train-labels.idx1-ubyte\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m cfg \u001b[38;5;241m=\u001b[39m Config(input_layer_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m     12\u001b[0m               output_layer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     13\u001b[0m               hidden_layer_num\u001b[38;5;241m=\u001b[39m hidden_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m               epochs\u001b[38;5;241m=\u001b[39mepochs\n\u001b[1;32m     20\u001b[0m               )\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 197\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(images_pth, labels_pth, cfg)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex position: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m(index))\n\u001b[1;32m    196\u001b[0m input_ly, label_ly \u001b[38;5;241m=\u001b[39m create_batch(cfg, index, f, l)\n\u001b[0;32m--> 197\u001b[0m \u001b[43mbatch_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 173\u001b[0m, in \u001b[0;36mbatch_learning\u001b[0;34m(input_batch, desired_output_batch, net, cfg)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(items))\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m#for stuff in items:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m      \u001b[38;5;66;03m#   print(type(stuff))\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m#    print(np.shape(stuff))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#print(len(useful))\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m#print(useful)\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[43mgwfioen\u001b[49m)\n\u001b[1;32m    176\u001b[0m w_delta_mean \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    179\u001b[0m b_delta_mean \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gwfioen' is not defined"
     ]
    }
   ],
   "source": [
    "image_size = 28**2\n",
    "batch_size = 100\n",
    "dataset_size = 60000\n",
    "hidden_layers = 3\n",
    "hidden_layer_size = 17\n",
    "learning_rate = 0.05\n",
    "epochs = 2\n",
    "images_path = \"MNIST/train-images.idx3-ubyte\"\n",
    "labels_path = \"MNIST/train-labels.idx1-ubyte\"\n",
    "\n",
    "cfg = Config(input_layer_size=image_size,\n",
    "              output_layer_size=10,\n",
    "              hidden_layer_num= hidden_layers,\n",
    "              hidden_layer_size = hidden_layer_size,\n",
    "              \n",
    "              learning_rate=learning_rate,\n",
    "              dataset_size=dataset_size,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs\n",
    "              )\n",
    "\n",
    "\n",
    "train(images_path, labels_path, cfg=cfg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_network(3,5,1,2)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,1,0]\n",
    "\n",
    "net.weights[1] = np.transpose(np.array([[0,0],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1],\n",
    "                           [0.3,-1],\n",
    "                           [0.1,0.1]]))\n",
    "net.weights[0] = (np.array([[1,1],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1]]))\n",
    "print(np.shape(net.weights[1]))\n",
    "\n",
    "net = forward(input,net)\n",
    "for index, items in enumerate(net.layers):\n",
    "    print(\"layer: \", index)\n",
    "    print(\"net layer size: \",np.shape(items), \"\\nand layer values: \", items)\n",
    "for index, items in enumerate(net.weights):\n",
    "    print(\"layer: \", index)\n",
    "    print(\"net weight layer size: \",np.shape(items), \"\\nand weight layer values: \", items)\n",
    "for index, items in enumerate(net.biases):\n",
    "    print(\"layer: \", index)\n",
    "    print(\"net bias layer size: \",np.shape(items), \"\\nand bias layer values: \", items)\n",
    "\n",
    "\n",
    "deltas_list = backprop_single(net, output)\n",
    "for index, items in enumerate(deltas_list[0]):\n",
    "    print(\"layer: \", index)\n",
    "    print(\"Weight change layer size: \",np.shape(items), \"\\nand weight change values: \", items)\n",
    "for index, items in enumerate(deltas_list[1]):\n",
    "    print(\"layer: \", index)\n",
    "    print(\"bias change layer size: \",np.shape(items), \"\\nand bias change values: \", items)\n",
    "for index, items in enumerate(deltas_list[2]):\n",
    "    print(\"layer: \", index)\n",
    "    print(\"activation change layer size: \",np.shape(items), \"\\nand activation change values: \", items)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
