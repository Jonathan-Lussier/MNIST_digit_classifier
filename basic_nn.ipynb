{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simaliar to the first attempt but with cleaner code, more flexible layer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "print(\"start!\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    input_layer_size: int\n",
    "    output_layer_size: int\n",
    "    hidden_layer_num : int\n",
    "    hidden_layer_size : int\n",
    "\n",
    "    dataset_size : int\n",
    "    learning_rate :float\n",
    "    batch_size : int\n",
    "    epochs : int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer(layer_size, dtype=np.float32, **kwargs):\n",
    "    layer = np.zeros(layer_size, dtype=dtype)\n",
    "    return(layer)\n",
    "\n",
    "def create_layer_weights(prev_ly_size, curr_ly_size, init=True):\n",
    "    if init:\n",
    "        ly_weights = np.random.rand(prev_ly_size, curr_ly_size)\n",
    "    else:\n",
    "        ly_weights = np.zeros(shape=(prev_ly_size, curr_ly_size))\n",
    "    return(ly_weights)\n",
    "\n",
    "def create_biases(ly_size, init=True):\n",
    "    if init:\n",
    "        biases = np.random.rand(ly_size)\n",
    "    else:\n",
    "        biases = np.zeros(ly_size)\n",
    "    return(biases)\n",
    "\n",
    "\n",
    "def label_to_output(int) -> np.ndarray:\n",
    "    outlayer = np.zeros(shape=(10), dtype=np.float32)\n",
    "    outlayer[int] = 1\n",
    "    return(outlayer)\n",
    "\n",
    "def label_vec_to_output(inputs: np.ndarray) -> np.ndarray:\n",
    "    # Create an identity matrix of size 10 and index into it\n",
    "    return np.eye(10, dtype=np.int8)[inputs]\n",
    "\n",
    "class n_network:\n",
    "    def __init__(self, layers, z_layers, biases, weights):\n",
    "        self.layers = layers\n",
    "        self.z_layers = z_layers\n",
    "        self.biases = biases\n",
    "        self.weights = weights\n",
    "\n",
    "def create_network(in_ly_size, out_ly_size, hidden_ly_num, hidden_ly_size):\n",
    "    in_layer = create_layer(in_ly_size)\n",
    "    hidden_lys = []\n",
    "    for index in range(hidden_ly_num):\n",
    "        hidden_lys.append(create_layer(hidden_ly_size))\n",
    "    out_layer = create_layer(out_ly_size)\n",
    "    network_lys = [in_layer, *hidden_lys, out_layer]\n",
    "\n",
    "\n",
    "    biases =[]\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        biases.append(create_biases(np.size(network_lys[i]), init=False))\n",
    "\n",
    "    weights = []\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        weights.append( create_layer_weights(np.size(network_lys[i-1]), np.size(network_lys[i]), init=True))\n",
    "\n",
    "    z_layers = deep_copy_mat_list(network_lys, False)\n",
    "\n",
    "    network = n_network(network_lys, z_layers, biases, weights)\n",
    "    return(network)\n",
    "\n",
    "\n",
    "def sigx(number):\n",
    "    #sigmond function\n",
    "    #todo: make a nativly vectorized verison of this \n",
    "    try:\n",
    "        sig_x = ( 1 / (1+ math.exp(-number)))\n",
    "        #this does not woprk for large negitive numbers \n",
    "        # due to rounding errors leading to devide by zero so we'll add error handling\n",
    "    except OverflowError:\n",
    "        sig_x = 0\n",
    "    return(sig_x)\n",
    "\n",
    "def sigmond(input):\n",
    "    #a more easily vectorizable sigmond function\n",
    "    return( 1/(1+np.exp(-input)))\n",
    "\n",
    "def sigmond_prime(input):\n",
    "\n",
    "    return(sigmond(input) * (1-sigmond(input)))\n",
    "\n",
    "def forward(input_vals:np.ndarray, net:n_network):\n",
    "    #first layer\n",
    "    input_vals = np.array(input_vals)\n",
    "    net.layers[0] = input_vals\n",
    "\n",
    "    for index in range(1, len(net.layers), 1):\n",
    "        net.layers[index] = np.dot(net.layers[(index-1)], net.weights[index-1]) + net.biases[index-1]\n",
    "        \n",
    "        #activation function\n",
    "        sigVec = np.vectorize(sigx)\n",
    "        net.layers[index] = sigVec(net.layers[index])\n",
    "    return(net)\n",
    "\n",
    "def node_delta(expected:np.array, actual:np.array):\n",
    "    delta = expected - actual\n",
    "    return(delta)\n",
    "\n",
    "def deep_copy_mat_list (list_to_copy, propigate_cell_vals=True):\n",
    "    #creates a copy of a list containing numpy arrays of varible size, ethier as empty arrays or with the same values\n",
    "    \n",
    "    new_list = []\n",
    "    if propigate_cell_vals:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(items)\n",
    "    else:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(np.zeros_like(items))       \n",
    "\n",
    "    return(new_list)\n",
    "\n",
    "\n",
    "def loss(network_output_ly, expected_ly):\n",
    "    #loss =  mean of all output neurons - the expected, squared\n",
    "    loss_vec = ((network_output_ly - expected_ly)**2) / 2\n",
    "    loss_sclr = np.sum((network_output_ly - expected_ly)**2)\n",
    "    return(loss_sclr, loss_vec)\n",
    "\n",
    "def backprop_single(net: n_network, expected_out_ly):\n",
    "    #does backprop for a single training example, duhh\n",
    "    #add learning rate??\n",
    "    \n",
    "    expected_out_ly = np.array(expected_out_ly)\n",
    "    w_delta = deep_copy_mat_list(net.weights, propigate_cell_vals=False)\n",
    "    b_delta = deep_copy_mat_list(net.biases, propigate_cell_vals=False)\n",
    "    a_delta = deep_copy_mat_list(net.layers, propigate_cell_vals=False)\n",
    "\n",
    "    first = True\n",
    "    for l in range(len(net.layers)-1, 0, -1):\n",
    "        #print(\"at l of:\", l)\n",
    "\n",
    "        if first:\n",
    "            #first(last) layer\n",
    "            first = False\n",
    "            a_delta_dir = expected_out_ly - net.layers[l]  \n",
    "            w_delta[l-1] = np.outer(net.layers[l-1], a_delta_dir)   \n",
    "            a_delta[l] = a_delta_dir\n",
    "        else:\n",
    "            a_delta_dir = a_delta[l] - net.layers[l]\n",
    "            w_delta[l-1] = np.outer(net.layers[l-1], a_delta_dir)   \n",
    "\n",
    "        a_delta[l-1] = np.dot(net.weights[l-1], net.layers[l])\n",
    "        a_delta[l-1] = a_delta[l-1] / a_delta[l-1].size\n",
    "    return(w_delta, b_delta)\n",
    "\n",
    "def create_batch(cfg:Config, batch_position, image_file, label_file):\n",
    "   \n",
    "    image_buffer = image_file.read(cfg.input_layer_size * cfg.batch_size)\n",
    "    input_batch = np.frombuffer(image_buffer, dtype=np.uint8).astype(np.float32)\n",
    "    input_batch = input_batch.reshape(cfg.batch_size, cfg.input_layer_size)\n",
    "    \n",
    "    label_buffer = label_file.read(cfg.batch_size)\n",
    "    label_batch = np.frombuffer(label_buffer, dtype=np.uint8).astype(np.int64)\n",
    "    Label_batch = label_vec_to_output(label_batch)\n",
    "\n",
    "    return(input_batch, Label_batch)\n",
    "    \n",
    "\n",
    "def batch_learning(input_batch:np.ndarray, desired_output_batch:np.ndarray, net:n_network, cfg:Config):\n",
    "    cfg.batch_size\n",
    "    w_del_batch = []\n",
    "    b_del_batch = []\n",
    "    batch_loss = np.ndarray((cfg.batch_size,))\n",
    "\n",
    "    for layers in net.weights:\n",
    "        w_del_batch.append(np.zeros((cfg.batch_size,) + layers.shape, dtype=np.float32)  )\n",
    "\n",
    "\n",
    "    for batch in range(cfg.batch_size):\n",
    "        net = forward(input_batch[batch], net)\n",
    "        w_del_batch_tmp, b_del_batch_tmp = backprop_single(net, desired_output_batch[batch])\n",
    "\n",
    "        for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "            w_del_batch[layer_num][batch,:,:] = w_del_batch_tmp[layer_num]\n",
    "        \n",
    "        loss_sclr, loss_vec = loss(net.layers[-1], desired_output_batch[batch])\n",
    "        batch_loss[batch] = loss_sclr\n",
    "    batch_loss = np.mean(batch_loss)\n",
    "    #print(\"batch_loss: \",batch_loss)\n",
    "\n",
    "\n",
    "\n",
    "    w_delta_mean = deep_copy_mat_list(net.weights, False)\n",
    "    for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "        w_delta_mean[layer_num] = np.mean(w_del_batch[layer_num], axis=0) * cfg.learning_rate #learning rate is here!\n",
    "    #    print(w_delta_mean[layer_num].shape)\n",
    "    #print(\"W_delta array: \\n\")\n",
    "    #for layers in w_delta_mean:\n",
    "    #    print(layers.shape)\n",
    "    #    print(layers)\n",
    "\n",
    "\n",
    "    b_delta_mean = []\n",
    "    descent_step = w_delta_mean, b_delta_mean\n",
    "    return(descent_step)\n",
    "\n",
    "\n",
    "def train(images_pth, labels_pth, cfg:Config):\n",
    "\n",
    "    net = create_network(cfg.input_layer_size, cfg.output_layer_size, cfg.hidden_layer_num, cfg.hidden_layer_size)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(\"epoch \", int(epoch), \"out of \", int(cfg.epochs))\n",
    "        f = open(images_pth, 'rb')\n",
    "        f.read(16)\n",
    "        l = open(labels_pth,'rb')\n",
    "        l.read(8)\n",
    "        for index in range(0, cfg.dataset_size, cfg.batch_size):\n",
    "            #print(\"index position: \", int(index))\n",
    "            input_ly, label_ly = create_batch(cfg, index, f, l)\n",
    "            #batch_learning(input_ly, label_ly, net,cfg)\n",
    "            W_delta, B_delta = batch_learning(input_ly, label_ly, net,cfg)\n",
    "            for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "                net.weights[layer_num] = net.weights[layer_num] + W_delta[layer_num]\n",
    "\n",
    "\n",
    "\n",
    "    #do batches here:\n",
    "    return(net)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 out of  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\MNIST_digit_classifier\\working_env\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2605: RuntimeWarning: overflow encountered in sigx (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 out of  5\n",
      "epoch  2 out of  5\n",
      "epoch  3 out of  5\n",
      "epoch  4 out of  5\n"
     ]
    }
   ],
   "source": [
    "image_size = 28**2\n",
    "batch_size = 100\n",
    "dataset_size = 60000\n",
    "hidden_layers = 3\n",
    "hidden_layer_size = 17\n",
    "learning_rate = 0.05\n",
    "epochs = 5\n",
    "images_path = \"MNIST/train-images.idx3-ubyte\"\n",
    "labels_path = \"MNIST/train-labels.idx1-ubyte\"\n",
    "\n",
    "cfg = Config(input_layer_size=image_size,\n",
    "              output_layer_size=10,\n",
    "              hidden_layer_num= hidden_layers,\n",
    "              hidden_layer_size = hidden_layer_size,\n",
    "              \n",
    "              learning_rate=learning_rate,\n",
    "              dataset_size=dataset_size,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs\n",
    "              )\n",
    "\n",
    "\n",
    "net = train(images_path, labels_path, cfg=cfg)\n",
    "weights_layer_1 = net.weights[0]\n",
    "weights_layer_2 = net.weights[1]\n",
    "weights_layer_3 = net.weights[2]\n",
    "weights_layer_4 = net.weights[3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"MNIST/t10k-images.idx3-ubyte\", 'rb')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 50\n",
    "choice = 10\n",
    "\n",
    "try:\n",
    "    f.read(16)\n",
    "except UnicodeDecodeError as error:\n",
    "    print(error)\n",
    "    print(\"utf-8 error is usally due to binary format\")\n",
    "\n",
    "\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "batch = data.reshape(num_images, 28*28)\n",
    "data = data.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(data[choice]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "l = open(\"MNIST/t10k-labels.idx1-ubyte\",'rb')\n",
    "l.read(8)\n",
    "buf = l.read(10000)\n",
    "labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "print(labels[choice])\n",
    "\n",
    "inlayer = batch[choice]\n",
    "net = forward(inlayer, net)\n",
    "print(net.layers[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "net = create_network(3,5,1,2)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,1,0]\n",
    "\n",
    "net.weights[1] = np.transpose(np.array([[0,0],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1],\n",
    "                           [0.3,-1],\n",
    "                           [0.1,0.1]]))\n",
    "net.weights[0] = (np.array([[1,1],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1]]))\n",
    "print(np.shape(net.weights[1]))\n",
    "\n",
    "net = forward(input,net)\n",
    "w_delta, b_delta = backprop_single(net, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_network(3,5,2,7)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,1,0]\n",
    "\n",
    "net = forward(input,net)\n",
    "w_delta, b_delta = backprop_single(net, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
