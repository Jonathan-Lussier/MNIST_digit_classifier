{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simaliar to the first attempt but with cleaner code, more flexible layer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"start!\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    input_layer_size: int\n",
    "    output_layer_size: int\n",
    "    hidden_layer_num : int\n",
    "    hidden_layer_size : int\n",
    "\n",
    "    dataset_size : int\n",
    "    learning_rate :float\n",
    "    batch_size : int\n",
    "    epochs : int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer(layer_size, dtype=np.float32, **kwargs):\n",
    "    layer = np.zeros(layer_size, dtype=dtype)\n",
    "    return(layer)\n",
    "\n",
    "def create_layer_weights(prev_ly_size, curr_ly_size, init=True):\n",
    "    if init:\n",
    "        #ly_weights = np.random.rand(prev_ly_size, curr_ly_size)\n",
    "        ly_weights = np.random.uniform(-1,1, size=(prev_ly_size, curr_ly_size))\n",
    "    else:\n",
    "        ly_weights = np.zeros(shape=(prev_ly_size, curr_ly_size))\n",
    "    return(ly_weights)\n",
    "\n",
    "def create_biases(ly_size, init=True):\n",
    "    if init:\n",
    "        biases = np.random.rand(ly_size)\n",
    "    else:\n",
    "        biases = np.zeros(ly_size)\n",
    "    return(biases)\n",
    "\n",
    "\n",
    "def label_to_output(int) -> np.ndarray:\n",
    "    outlayer = np.zeros(shape=(10), dtype=np.float32)\n",
    "    outlayer[int] = 1\n",
    "    return(outlayer)\n",
    "\n",
    "def label_vec_to_output(inputs: np.ndarray) -> np.ndarray:\n",
    "    # Create an identity matrix of size 10 and index into it\n",
    "    return np.eye(10, dtype=np.int8)[inputs]\n",
    "\n",
    "class n_network:\n",
    "    def __init__(self, layers, z_layers, biases, weights):\n",
    "        self.layers = layers\n",
    "        self.z_layers = z_layers\n",
    "        self.biases = biases\n",
    "        self.weights = weights\n",
    "\n",
    "def create_network(in_ly_size, out_ly_size, hidden_ly_num, hidden_ly_size):\n",
    "    in_layer = create_layer(in_ly_size)\n",
    "    hidden_lys = []\n",
    "    for index in range(hidden_ly_num):\n",
    "        hidden_lys.append(create_layer(hidden_ly_size))\n",
    "    out_layer = create_layer(out_ly_size)\n",
    "    network_lys = [in_layer, *hidden_lys, out_layer]\n",
    "    \n",
    "    z_layers = deep_copy_mat_list(network_lys, False)\n",
    "\n",
    "    biases =[]\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        biases.append(create_biases(np.size(network_lys[i]), init=False))\n",
    "\n",
    "    weights = []\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        weights.append( create_layer_weights(np.size(network_lys[i-1]), np.size(network_lys[i]), init=True))\n",
    "\n",
    "    network = n_network(network_lys, z_layers, biases, weights)\n",
    "    return(network)\n",
    "\n",
    "\n",
    "def sigx(number):\n",
    "    #sigmond function\n",
    "    try:\n",
    "        sig_x = ( 1 / (1+ math.exp(-number)))\n",
    "        #this does not work for large negitive numbers \n",
    "        # due to rounding errors leading to devide by zero so we'll add error handling\n",
    "    except OverflowError:\n",
    "        sig_x = 0\n",
    "    return(sig_x)\n",
    "\n",
    "def sigmond(input):\n",
    "    #a more easily vectorizable sigmond function\n",
    "    return( 1/(1+np.exp(-input)))\n",
    "\n",
    "def sigmond_prime(input):\n",
    "    return(sigmond(input) * (1-sigmond(input)))\n",
    "\n",
    "def forward(input_vals:np.ndarray, net:n_network):\n",
    "    #first layer\n",
    "    input_vals = np.array(input_vals)\n",
    "    net.layers[0] = input_vals\n",
    "\n",
    "    for index in range(1, len(net.layers), 1):\n",
    "        net.z_layers[index] = np.dot(net.layers[(index-1)], net.weights[index-1]) + net.biases[index-1]\n",
    "        net.layers[index] = sigmond(net.z_layers[index])  #activation function\n",
    "    return(net)\n",
    "\n",
    "def node_delta(expected:np.array, actual:np.array):\n",
    "    delta = expected - actual\n",
    "    return(delta)\n",
    "\n",
    "def deep_copy_mat_list (list_to_copy, propigate_cell_vals=True):\n",
    "    #creates a copy of a list containing numpy arrays of varible size, ethier as empty arrays or with the same values\n",
    "    \n",
    "    new_list = []\n",
    "    if propigate_cell_vals:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(items)\n",
    "    else:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(np.zeros_like(items))       \n",
    "\n",
    "    return(new_list)\n",
    "\n",
    "\n",
    "def loss(network_output_ly, expected_ly):\n",
    "    #loss =  mean of all output neurons - the expected, squared\n",
    "    loss_vec = ((network_output_ly - expected_ly)**2) / 2\n",
    "    loss_sclr = np.sum((network_output_ly - expected_ly)**2)\n",
    "    return(loss_sclr, loss_vec)\n",
    "\n",
    "def backprop_single(net: n_network, expected_out_ly):\n",
    "    #does backprop for a single training example, duhh\n",
    "    first = True\n",
    "    expected_out_ly = np.array(expected_out_ly)\n",
    "    w_delta = deep_copy_mat_list(net.weights, propigate_cell_vals=False)\n",
    "    b_delta = deep_copy_mat_list(net.biases, propigate_cell_vals=False)\n",
    "    a_error = deep_copy_mat_list(net.layers, propigate_cell_vals=False)\n",
    "\n",
    "    for l in range(len(net.layers)-1, 0, -1):\n",
    "        w = l - 1\n",
    "\n",
    "        if first:\n",
    "            #first(last) layer\n",
    "            first = False\n",
    "            a_delta_dir = expected_out_ly - net.layers[l]\n",
    "            a_error[l] = a_delta_dir * sigmond_prime(net.z_layers[l])\n",
    "            a_error[l-1] = np.dot((net.weights[w]) , a_error[l]) * sigmond_prime(net.z_layers[l-1])\n",
    "            w_delta[w] =  np.outer(net.layers[l-1], a_error[l])\n",
    "            b_delta[w] = a_error[l]\n",
    "\n",
    "        else:\n",
    "            a_error[l] = np.dot((net.weights[w+1]) , a_error[l+1]) * sigmond_prime(net.z_layers[l])\n",
    "            w_delta[w] =  np.outer(net.layers[l-1], a_error[l])\n",
    "            b_delta[w] = a_error[l]\n",
    "\n",
    "    return(w_delta, b_delta)\n",
    "\n",
    "def create_batch(cfg:Config, image_file, label_file):\n",
    "   #this takes the raw file objects and reads an appropreatly sized batch in numpy arrays\n",
    "\n",
    "    image_buffer = image_file.read(cfg.input_layer_size * cfg.batch_size)\n",
    "    input_batch = np.frombuffer(image_buffer, dtype=np.uint8).astype(np.float32)\n",
    "    input_batch = input_batch.reshape(cfg.batch_size, cfg.input_layer_size)\n",
    "    label_buffer = label_file.read(cfg.batch_size)\n",
    "    label_batch = np.frombuffer(label_buffer, dtype=np.uint8).astype(np.int64)\n",
    "    Label_batch = label_vec_to_output(label_batch)\n",
    "\n",
    "    return(input_batch, Label_batch)\n",
    "    \n",
    "def batch_learning(input_batch:np.ndarray, desired_output_batch:np.ndarray, net:n_network, cfg:Config):\n",
    "    #This runs the network across every training example in a single batch and averages the results, returning a decent step and the loss across the batch\n",
    "\n",
    "    cfg.batch_size\n",
    "    w_del_batch = []\n",
    "    b_del_batch = []\n",
    "    batch_loss = np.ndarray((cfg.batch_size,))\n",
    "\n",
    "    for layers in net.weights:\n",
    "        w_del_batch.append(np.zeros((cfg.batch_size,) + layers.shape, dtype=np.float32)  )\n",
    "    for layers in net.biases:\n",
    "        b_del_batch.append(np.zeros((cfg.batch_size,) + layers.shape, dtype=np.float32)  )\n",
    "\n",
    "\n",
    "    for batch in range(cfg.batch_size):\n",
    "        net = forward(input_batch[batch], net)\n",
    "        w_del_batch_tmp, b_del_batch_tmp = backprop_single(net, desired_output_batch[batch])\n",
    "\n",
    "        for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "            w_del_batch[layer_num][batch,:,:] = w_del_batch_tmp[layer_num]\n",
    "            b_del_batch[layer_num][batch,:] = b_del_batch_tmp[layer_num]\n",
    "        \n",
    "        loss_sclr, loss_vec = loss(net.layers[-1], desired_output_batch[batch])\n",
    "        batch_loss[batch] = loss_sclr\n",
    "    batch_loss = np.mean(batch_loss)\n",
    "\n",
    "    w_delta_mean = deep_copy_mat_list(net.weights, False)\n",
    "    b_delta_mean = deep_copy_mat_list(net.biases, False)\n",
    "    for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "        w_delta_mean[layer_num] = np.mean(w_del_batch[layer_num], axis=0) * cfg.learning_rate #learning rate is here!\n",
    "        b_delta_mean[layer_num] = np.mean(b_del_batch[layer_num], axis=0) * cfg.learning_rate\n",
    "\n",
    "    descent_step = w_delta_mean, b_delta_mean\n",
    "    return(descent_step, batch_loss)\n",
    "\n",
    "def train(images_pth, labels_pth, cfg:Config):\n",
    "    #the core training loop that trains through epochs.  \n",
    "    # requires an images path, labels path(ideally located in the same directory as this code), and cfg containing hyperparameters\n",
    "\n",
    "    #plotting stuff \n",
    "    x_data, y_data = [],[]\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    net = create_network(cfg.input_layer_size, cfg.output_layer_size, cfg.hidden_layer_num, cfg.hidden_layer_size)\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(\"epoch \", int(epoch), \"out of \", int(cfg.epochs))\n",
    "        f = open(images_pth, 'rb')\n",
    "        f.read(16)\n",
    "        l = open(labels_pth,'rb')\n",
    "        l.read(8)\n",
    "        for index in range(0, cfg.dataset_size, cfg.batch_size):\n",
    "            #print(\"index position: \", int(index))\n",
    "            input_ly, label_ly = create_batch(cfg, f, l)\n",
    "            #batch_learning(input_ly, label_ly, net,cfg)\n",
    "            (W_delta, B_delta), loss = batch_learning(input_ly, label_ly, net,cfg)\n",
    "            for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "                net.weights[layer_num] = net.weights[layer_num] + W_delta[layer_num]\n",
    "                net.biases[layer_num] = net.biases[layer_num] + B_delta[layer_num]\n",
    "        \n",
    "        #plotting stuff\n",
    "        x_data.append(epoch)\n",
    "        y_data.append(loss)\n",
    "        clear_output(wait=True)        # Clear previous plot\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(x_data, y_data, 'bo-')\n",
    "        plt.title(\"Epoch vs Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss at epoch end\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return(net, (x_data, y_data))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights_biases(net:n_network):\n",
    "#save the weights and biases\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n",
    "    file_name = f\"weights/nn_training_{timestamp}.pkl\"\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(net, f)\n",
    "    return(timestamp, file_name)\n",
    "\n",
    "def save_performance(x_epoch, y_loss, cfg:Config, timestamp):\n",
    "    #save hyperparams and preformance data to a csv\n",
    "    import csv\n",
    "    #[\"run ID\", \"timestamp\", \"number of hidden layers\", \"hidden layer size\", \"LR\", \"Epochs\", \"Batch size\"]\n",
    "    from datetime import datetime\n",
    "    #timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_id = f\"{cfg.hidden_layer_num}{cfg.hidden_layer_size}{cfg.learning_rate}{cfg.epochs}{cfg.batch_size}\"\n",
    "    data = [run_id, timestamp, cfg.hidden_layer_num, cfg.hidden_layer_size, cfg.learning_rate, cfg.epochs, cfg.batch_size]\n",
    "\n",
    "    results_name = 'results.csv'\n",
    "    with open(results_name, mode='a', newline='') as file:\n",
    "        writer =  csv.writer(file)\n",
    "        writer.writerow(data)\n",
    "\n",
    "    plot_f_name = f\"results/{run_id}_{timestamp}_plots.csv\"\n",
    "    print(type(y_loss))\n",
    "    plots = [[x_epoch],[y_loss]]\n",
    "    with open(plot_f_name, mode='w', newline='') as file:\n",
    "        writer =  csv.writer(file)\n",
    "        for index, value in enumerate(x_epoch):\n",
    "            writer.writerow([int(x_epoch[index]), (y_loss[index])])\n",
    "    return(run_id, results_name,plot_f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28**2\n",
    "batch_size = 100\n",
    "dataset_size = 60000\n",
    "hidden_layers = 3\n",
    "hidden_layer_size = 17\n",
    "learning_rate = 0.5\n",
    "epochs = 2\n",
    "images_path = \"MNIST/train-images.idx3-ubyte\"\n",
    "labels_path = \"MNIST/train-labels.idx1-ubyte\"\n",
    "\n",
    "cfg = Config(input_layer_size=image_size,\n",
    "              output_layer_size=10,\n",
    "              hidden_layer_num= hidden_layers,\n",
    "              hidden_layer_size = hidden_layer_size,\n",
    "              \n",
    "              learning_rate=learning_rate,\n",
    "              dataset_size=dataset_size,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs\n",
    "              )\n",
    "\n",
    "net, (x_epoch, y_loss) = train(images_path, labels_path, cfg=cfg)\n",
    "timestamp, f_name = save_weights_biases(net)\n",
    "run_id, results_f_name, plot_f_name = save_performance(x_epoch,y_loss, cfg=cfg, timestamp=timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for h_layers in range(1,4,1):\n",
    "    #print(\"number of hidden layers: \", h_layers)\n",
    "\n",
    "for lr in range(25, 1001, 50):\n",
    "    print(\"learning rate: \", lr/1000)\n",
    "    image_size = 28**2\n",
    "    batch_size = 100\n",
    "    dataset_size = 60000\n",
    "    hidden_layers = 3\n",
    "    hidden_layer_size = 15\n",
    "    learning_rate = lr/1000\n",
    "    epochs = 50\n",
    "    images_path = \"MNIST/train-images.idx3-ubyte\"\n",
    "    labels_path = \"MNIST/train-labels.idx1-ubyte\"\n",
    "\n",
    "    cfg = Config(input_layer_size=image_size,\n",
    "                output_layer_size=10,\n",
    "                hidden_layer_num= hidden_layers,\n",
    "                hidden_layer_size = hidden_layer_size,\n",
    "                \n",
    "                learning_rate=learning_rate,\n",
    "                dataset_size=dataset_size,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs\n",
    "                )\n",
    "\n",
    "    net, (x_epoch, y_loss) = train(images_path, labels_path, cfg=cfg)\n",
    "    timestamp, f_name = save_weights_biases(net)\n",
    "    run_id, results_f_name, plot_f_name = save_performance(x_epoch,y_loss, cfg=cfg, timestamp=timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"MNIST/t10k-images.idx3-ubyte\", 'rb')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 50\n",
    "choice = 15\n",
    "\n",
    "try:\n",
    "    f.read(16)\n",
    "except UnicodeDecodeError as error:\n",
    "    print(error)\n",
    "    print(\"utf-8 error is usally due to binary format\")\n",
    "\n",
    "\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "batch = data.reshape(num_images, 28*28)\n",
    "data = data.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(data[choice]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "l = open(\"MNIST/t10k-labels.idx1-ubyte\",'rb')\n",
    "l.read(8)\n",
    "buf = l.read(10000)\n",
    "labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "print(\"image key: \", labels[choice])\n",
    "\n",
    "inlayer = batch[choice]\n",
    "net = forward(inlayer, net)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(\"network's guess: \", np.argmax(net.layers[-1]))\n",
    "\n",
    "print(net.layers[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"n_net_with_weights.pkl\", \"wb\") as f:\n",
    "    pickle.dump(net, f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"n_net_with_weights.pkl\", \"rb\") as f:\n",
    "    net_loaded = pickle.load(f)\n",
    "\n",
    "f = open(\"MNIST/t10k-images.idx3-ubyte\", 'rb')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 50\n",
    "choice = 15\n",
    "\n",
    "try:\n",
    "    f.read(16)\n",
    "except UnicodeDecodeError as error:\n",
    "    print(error)\n",
    "    print(\"utf-8 error is usally due to binary format\")\n",
    "\n",
    "\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "batch = data.reshape(num_images, 28*28)\n",
    "data = data.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(data[choice]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "l = open(\"MNIST/t10k-labels.idx1-ubyte\",'rb')\n",
    "l.read(8)\n",
    "buf = l.read(10000)\n",
    "labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "print(labels[choice])\n",
    "\n",
    "inlayer = batch[choice]\n",
    "net_loaded = forward(inlayer, net_loaded)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(net_loaded.layers[-1])\n",
    "print(np.argmax(net_loaded.layers[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_network(3,5,1,2)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,1,0]\n",
    "\n",
    "net.weights[1] = np.transpose(np.array([[0,0],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1],\n",
    "                           [0.3,-1],\n",
    "                           [0.1,0.1]]))\n",
    "net.weights[0] = (np.array([[1,1],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1]]))\n",
    "print(np.shape(net.weights[1]))\n",
    "\n",
    "net = forward(input,net)\n",
    "w_delta, b_delta = backprop_single(net, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_network(3,5,2,2)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,0,0]\n",
    "\n",
    "net = forward(input,net)\n",
    "w_delta, b_delta = backprop_single(net, output)\n",
    "\n",
    "for index in range(len(net.layers)):\n",
    "    print(\"index position: \", index)\n",
    "\n",
    "    print(\"layer size and shape: \", net.layers[index].shape)\n",
    "\n",
    "    try:\n",
    "        print(\"weights size and shape: \", net.weights[index].shape)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        print(\"bias size and shape: \", net.biases[index].shape)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(net.weights[-1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
