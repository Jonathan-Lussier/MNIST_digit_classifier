{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simaliar to the first attempt but with cleaner code, more flexible layer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "print(\"start!\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    input_layer_size: int\n",
    "    output_layer_size: int\n",
    "    hidden_layer_num : int\n",
    "    hidden_layer_size : int\n",
    "\n",
    "    dataset_size : int\n",
    "    learning_rate :float\n",
    "    batch_size : int\n",
    "    epochs : int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer(layer_size, dtype=np.float32, **kwargs):\n",
    "    layer = np.zeros(layer_size, dtype=dtype)\n",
    "    return(layer)\n",
    "\n",
    "def create_layer_weights(prev_ly_size, curr_ly_size, init=True):\n",
    "    if init:\n",
    "        ly_weights = np.random.rand(prev_ly_size, curr_ly_size)\n",
    "    else:\n",
    "        ly_weights = np.zeros(shape=(prev_ly_size, curr_ly_size))\n",
    "    return(ly_weights)\n",
    "\n",
    "def create_biases(ly_size, init=True):\n",
    "    if init:\n",
    "        biases = np.random.rand(ly_size)\n",
    "    else:\n",
    "        biases = np.zeros(ly_size)\n",
    "    return(biases)\n",
    "\n",
    "\n",
    "def label_to_output(int) -> np.ndarray:\n",
    "    outlayer = np.zeros(shape=(10), dtype=np.float32)\n",
    "    outlayer[int] = 1\n",
    "    return(outlayer)\n",
    "\n",
    "def label_vec_to_output(inputs: np.ndarray) -> np.ndarray:\n",
    "    # Create an identity matrix of size 10 and index into it\n",
    "    return np.eye(10, dtype=np.int8)[inputs]\n",
    "\n",
    "class n_network:\n",
    "    def __init__(self, layers, z_layers, biases, weights):\n",
    "        self.layers = layers\n",
    "        self.z_layers = z_layers\n",
    "        self.biases = biases\n",
    "        self.weights = weights\n",
    "\n",
    "def create_network(in_ly_size, out_ly_size, hidden_ly_num, hidden_ly_size):\n",
    "    in_layer = create_layer(in_ly_size)\n",
    "    hidden_lys = []\n",
    "    for index in range(hidden_ly_num):\n",
    "        hidden_lys.append(create_layer(hidden_ly_size))\n",
    "    out_layer = create_layer(out_ly_size)\n",
    "    network_lys = [in_layer, *hidden_lys, out_layer]\n",
    "\n",
    "\n",
    "    biases =[]\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        biases.append(create_biases(np.size(network_lys[i]), init=False))\n",
    "\n",
    "    weights = []\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        weights.append( create_layer_weights(np.size(network_lys[i-1]), np.size(network_lys[i]), init=True))\n",
    "\n",
    "    z_layers = deep_copy_mat_list(network_lys, False)\n",
    "\n",
    "    network = n_network(network_lys, z_layers, biases, weights)\n",
    "    return(network)\n",
    "\n",
    "\n",
    "def sigx(number):\n",
    "    #sigmond function\n",
    "    #todo: make a nativly vectorized verison of this \n",
    "    try:\n",
    "        sig_x = ( 1 / (1+ math.exp(-number)))\n",
    "        #this does not woprk for large negitive numbers \n",
    "        # due to rounding errors leading to devide by zero so we'll add error handling\n",
    "    except OverflowError:\n",
    "        sig_x = 0\n",
    "    return(sig_x)\n",
    "\n",
    "def sigmond(input):\n",
    "    #a more easily vectorizable sigmond function\n",
    "    return( 1/(1+np.exp(-input)))\n",
    "\n",
    "def sigmond_prime(input):\n",
    "\n",
    "    return(sigmond(input) * (1-sigmond(input)))\n",
    "\n",
    "def forward(input_vals:np.ndarray, net:n_network):\n",
    "    #first layer\n",
    "    input_vals = np.array(input_vals)\n",
    "    net.layers[0] = input_vals\n",
    "\n",
    "    for index in range(1, len(net.layers), 1):\n",
    "        net.layers[index] = np.dot(net.layers[(index-1)], net.weights[index-1]) + net.biases[index-1]\n",
    "        \n",
    "        #activation function\n",
    "        net.layers[index] = sigmond(net.layers[index])\n",
    "    return(net)\n",
    "\n",
    "def node_delta(expected:np.array, actual:np.array):\n",
    "    delta = expected - actual\n",
    "    return(delta)\n",
    "\n",
    "def deep_copy_mat_list (list_to_copy, propigate_cell_vals=True):\n",
    "    #creates a copy of a list containing numpy arrays of varible size, ethier as empty arrays or with the same values\n",
    "    \n",
    "    new_list = []\n",
    "    if propigate_cell_vals:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(items)\n",
    "    else:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(np.zeros_like(items))       \n",
    "\n",
    "    return(new_list)\n",
    "\n",
    "\n",
    "def loss(network_output_ly, expected_ly):\n",
    "    #loss =  mean of all output neurons - the expected, squared\n",
    "    loss_vec = ((network_output_ly - expected_ly)**2) / 2\n",
    "    loss_sclr = np.sum((network_output_ly - expected_ly)**2)\n",
    "    return(loss_sclr, loss_vec)\n",
    "\n",
    "def backprop_single(net: n_network, expected_out_ly):\n",
    "    #does backprop for a single training example, duhh\n",
    "    #add learning rate??\n",
    "    \n",
    "    expected_out_ly = np.array(expected_out_ly)\n",
    "    w_delta = deep_copy_mat_list(net.weights, propigate_cell_vals=False)\n",
    "    b_delta = deep_copy_mat_list(net.biases, propigate_cell_vals=False)\n",
    "    a_error = deep_copy_mat_list(net.layers, propigate_cell_vals=False)\n",
    "\n",
    "    first = True\n",
    "    for l in range(len(net.layers)-1, 0, -1):\n",
    "        w = l - 1\n",
    "        #print(\"l: \", l, \"w: \", w)\n",
    "\n",
    "        if first:\n",
    "            #first(last) layer\n",
    "            first = False\n",
    "            a_delta_dir = expected_out_ly - net.layers[l]\n",
    "            #print(\"a_delta_dir: \", len(a_delta_dir))\n",
    "            a_error[l] = a_delta_dir * sigmond_prime(net.z_layers[l])\n",
    "            #print(\"a_error_of l: \", len(a_error[l]))\n",
    "\n",
    "            a_error[l-1] = np.dot((net.weights[w]) , a_error[l]) * sigmond_prime(net.z_layers[l-1])\n",
    "            #print(\"a_error_of l-1: \", a_error[l-1].shape)\n",
    "\n",
    "            w_delta[w] =  np.outer(net.layers[l-1], a_error[l])\n",
    "            #print(\"w_delta of w: \", np.shape(w_delta[w]), np.shape(net.weights[w]))\n",
    "\n",
    "            b_delta[w] = a_error[w]\n",
    "\n",
    "        else:\n",
    "\n",
    "            a_error[l] = np.dot((net.weights[w+1]) , a_error[l+1]) * sigmond_prime(net.z_layers[l])\n",
    "            w_delta[w] =  np.outer(net.layers[l-1], a_error[l])\n",
    "            b_delta[w] = a_error[w]\n",
    "            #print(\"shape of b_delta: \", np.shape(b_delta[w]))\n",
    "\n",
    "    return(w_delta, b_delta)\n",
    "\n",
    "def backprop_single_OLD(net: n_network, expected_out_ly):\n",
    "    #does backprop for a single training example, duhh\n",
    "    #add learning rate??\n",
    "    \n",
    "    expected_out_ly = np.array(expected_out_ly)\n",
    "    w_delta = deep_copy_mat_list(net.weights, propigate_cell_vals=False)\n",
    "    b_delta = deep_copy_mat_list(net.biases, propigate_cell_vals=False)\n",
    "    a_delta = deep_copy_mat_list(net.layers, propigate_cell_vals=False)\n",
    "\n",
    "    first = True\n",
    "    for l in range(len(net.layers)-1, 0, -1):\n",
    "        #print(\"at l of:\", l)\n",
    "\n",
    "        if first:\n",
    "            #first(last) layer\n",
    "            first = False\n",
    "            a_delta_dir = expected_out_ly - net.layers[l]  \n",
    "            w_delta[l-1] = np.outer(net.layers[l-1], a_delta_dir)   \n",
    "            a_delta[l] = a_delta_dir\n",
    "        else:\n",
    "            a_delta_dir = a_delta[l] - net.layers[l]\n",
    "            w_delta[l-1] = np.outer(net.layers[l-1], a_delta_dir)   \n",
    "\n",
    "        a_delta[l-1] = np.dot(net.weights[l-1], net.layers[l])\n",
    "        a_delta[l-1] = a_delta[l-1] / a_delta[l-1].size\n",
    "    return(w_delta, b_delta)\n",
    "\n",
    "def create_batch(cfg:Config, batch_position, image_file, label_file):\n",
    "   \n",
    "    image_buffer = image_file.read(cfg.input_layer_size * cfg.batch_size)\n",
    "    input_batch = np.frombuffer(image_buffer, dtype=np.uint8).astype(np.float32)\n",
    "    input_batch = input_batch.reshape(cfg.batch_size, cfg.input_layer_size)\n",
    "    \n",
    "    label_buffer = label_file.read(cfg.batch_size)\n",
    "    label_batch = np.frombuffer(label_buffer, dtype=np.uint8).astype(np.int64)\n",
    "    Label_batch = label_vec_to_output(label_batch)\n",
    "\n",
    "    return(input_batch, Label_batch)\n",
    "    \n",
    "def batch_learning(input_batch:np.ndarray, desired_output_batch:np.ndarray, net:n_network, cfg:Config):\n",
    "    cfg.batch_size\n",
    "    w_del_batch = []\n",
    "    b_del_batch = []\n",
    "    batch_loss = np.ndarray((cfg.batch_size,))\n",
    "\n",
    "    for layers in net.weights:\n",
    "        w_del_batch.append(np.zeros((cfg.batch_size,) + layers.shape, dtype=np.float32)  )\n",
    "\n",
    "\n",
    "    for batch in range(cfg.batch_size):\n",
    "        net = forward(input_batch[batch], net)\n",
    "        w_del_batch_tmp, b_del_batch_tmp = backprop_single(net, desired_output_batch[batch])\n",
    "\n",
    "        for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "            w_del_batch[layer_num][batch,:,:] = w_del_batch_tmp[layer_num]\n",
    "        \n",
    "        loss_sclr, loss_vec = loss(net.layers[-1], desired_output_batch[batch])\n",
    "        batch_loss[batch] = loss_sclr\n",
    "    batch_loss = np.mean(batch_loss)\n",
    "    print(\"batch_loss: \",batch_loss)\n",
    "\n",
    "\n",
    "\n",
    "    w_delta_mean = deep_copy_mat_list(net.weights, False)\n",
    "    for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "        w_delta_mean[layer_num] = np.mean(w_del_batch[layer_num], axis=0) * cfg.learning_rate #learning rate is here!\n",
    "    #    print(w_delta_mean[layer_num].shape)\n",
    "    #print(\"W_delta array: \\n\")\n",
    "    #for layers in w_delta_mean:\n",
    "    #    print(layers.shape)\n",
    "    #    print(layers)\n",
    "\n",
    "\n",
    "    b_delta_mean = []\n",
    "    descent_step = w_delta_mean, b_delta_mean\n",
    "    return(descent_step)\n",
    "\n",
    "def train(images_pth, labels_pth, cfg:Config):\n",
    "\n",
    "    net = create_network(cfg.input_layer_size, cfg.output_layer_size, cfg.hidden_layer_num, cfg.hidden_layer_size)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(\"epoch \", int(epoch), \"out of \", int(cfg.epochs))\n",
    "        f = open(images_pth, 'rb')\n",
    "        f.read(16)\n",
    "        l = open(labels_pth,'rb')\n",
    "        l.read(8)\n",
    "        for index in range(0, cfg.dataset_size, cfg.batch_size):\n",
    "            #print(\"index position: \", int(index))\n",
    "            input_ly, label_ly = create_batch(cfg, index, f, l)\n",
    "            #batch_learning(input_ly, label_ly, net,cfg)\n",
    "            W_delta, B_delta = batch_learning(input_ly, label_ly, net,cfg)\n",
    "            for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "                net.weights[layer_num] = net.weights[layer_num] + W_delta[layer_num]\n",
    "\n",
    "\n",
    "\n",
    "    #do batches here:\n",
    "    return(net)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 out of  2\n",
      "batch_loss:  8.990142413134784\n",
      "batch_loss:  8.245562108736628\n",
      "batch_loss:  4.133542436641566\n",
      "batch_loss:  1.609149983781368\n",
      "batch_loss:  1.0900382119216123\n",
      "batch_loss:  0.9612034147287717\n",
      "batch_loss:  0.9270294938734607\n",
      "batch_loss:  0.9188776842276262\n",
      "batch_loss:  0.9075254138642751\n",
      "batch_loss:  0.9026539549916797\n",
      "batch_loss:  0.900931371184819\n",
      "batch_loss:  0.9005759876907713\n",
      "batch_loss:  0.9018424987788448\n",
      "batch_loss:  0.9036397908263892\n",
      "batch_loss:  0.9013072895759422\n",
      "batch_loss:  0.901671850710231\n",
      "batch_loss:  0.8988288486830354\n",
      "batch_loss:  0.9031964513506435\n",
      "batch_loss:  0.8989729388245203\n",
      "batch_loss:  0.8997119355285786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6921/3681864672.py:72: RuntimeWarning: overflow encountered in exp\n",
      "  return( 1/(1+np.exp(-input)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss:  0.9028062237079085\n",
      "batch_loss:  0.8999737360456759\n",
      "batch_loss:  0.8991290544515477\n",
      "batch_loss:  0.9034602743189393\n",
      "batch_loss:  0.8964940185521768\n",
      "batch_loss:  0.9024670079826045\n",
      "batch_loss:  0.9032505358810786\n",
      "batch_loss:  0.9004098064992475\n",
      "batch_loss:  0.8969309872181515\n",
      "batch_loss:  0.9030005895140489\n",
      "batch_loss:  0.9059176073184538\n",
      "batch_loss:  0.8999592472999209\n",
      "batch_loss:  0.9027090472674874\n",
      "batch_loss:  0.9017650253713108\n",
      "batch_loss:  0.9014809377873259\n",
      "batch_loss:  0.9020552386345174\n",
      "batch_loss:  0.9022325510764175\n",
      "batch_loss:  0.9004966730223807\n",
      "batch_loss:  0.9005339353999147\n",
      "batch_loss:  0.8990840367440731\n",
      "batch_loss:  0.8993718807473126\n",
      "batch_loss:  0.9027199127892844\n",
      "batch_loss:  0.9021087567862865\n",
      "batch_loss:  0.8984005670529115\n",
      "batch_loss:  0.904453106677891\n",
      "batch_loss:  0.8988506328296759\n",
      "batch_loss:  0.9039762814150446\n",
      "batch_loss:  0.8998714321638198\n",
      "batch_loss:  0.8986677022153059\n",
      "batch_loss:  0.9006803671432259\n",
      "batch_loss:  0.8999892554435762\n",
      "batch_loss:  0.9002986375872704\n",
      "batch_loss:  0.9009087056718585\n",
      "batch_loss:  0.8995325512506777\n",
      "batch_loss:  0.9009041645572812\n",
      "batch_loss:  0.8984805866393765\n",
      "batch_loss:  0.9000095514300186\n",
      "batch_loss:  0.9016609508823272\n",
      "batch_loss:  0.9001820672775932\n",
      "batch_loss:  0.8976267795713129\n",
      "batch_loss:  0.8991363939360083\n",
      "batch_loss:  0.9012188387629856\n",
      "batch_loss:  0.900036716546752\n",
      "batch_loss:  0.9015742753906824\n",
      "batch_loss:  0.900161467678728\n",
      "batch_loss:  0.8994976542862869\n",
      "batch_loss:  0.9024742261592905\n",
      "batch_loss:  0.8996287770650012\n",
      "batch_loss:  0.901132092109528\n",
      "batch_loss:  0.9002220121681234\n",
      "batch_loss:  0.8996973465309378\n",
      "batch_loss:  0.9001989208469078\n",
      "batch_loss:  0.9019117579916335\n",
      "batch_loss:  0.9021200062859823\n",
      "batch_loss:  0.8987069370949198\n",
      "batch_loss:  0.9007727350606415\n",
      "batch_loss:  0.8979811136775424\n",
      "batch_loss:  0.8988348110104798\n",
      "batch_loss:  0.8996357154594529\n",
      "batch_loss:  0.9022940921158547\n",
      "batch_loss:  0.9004370257502433\n",
      "batch_loss:  0.8974661407710185\n",
      "batch_loss:  0.9002926673712592\n",
      "batch_loss:  0.9007617953100404\n",
      "batch_loss:  0.9045750256619957\n",
      "batch_loss:  0.8971996353826489\n",
      "batch_loss:  0.9004110777611422\n",
      "batch_loss:  0.9038836482883894\n",
      "batch_loss:  0.9002172102982938\n",
      "batch_loss:  0.8986661054862268\n",
      "batch_loss:  0.9018787751562894\n",
      "batch_loss:  0.9014859250504557\n",
      "batch_loss:  0.8995833587048148\n",
      "batch_loss:  0.9057852977217778\n",
      "batch_loss:  0.9005917563162423\n",
      "batch_loss:  0.9007457936302221\n",
      "batch_loss:  0.8996063889222029\n",
      "batch_loss:  0.903097027053339\n",
      "batch_loss:  0.9009166801744982\n",
      "batch_loss:  0.8995893449318002\n",
      "batch_loss:  0.898442604756927\n",
      "batch_loss:  0.9017588797998627\n",
      "batch_loss:  0.9003862742384521\n",
      "batch_loss:  0.8998046238018368\n",
      "batch_loss:  0.9023837214427216\n",
      "batch_loss:  0.900783679466166\n",
      "batch_loss:  0.9011993990939448\n",
      "batch_loss:  0.9039131515716953\n",
      "batch_loss:  0.8992090611025586\n",
      "batch_loss:  0.90043660641966\n",
      "batch_loss:  0.9001839861758231\n",
      "batch_loss:  0.9023541384031191\n",
      "batch_loss:  0.8981628455388767\n",
      "batch_loss:  0.9020069704364319\n",
      "batch_loss:  0.9016777263523105\n",
      "batch_loss:  0.9009096791110801\n",
      "batch_loss:  0.8986570038875428\n",
      "batch_loss:  0.9027184502597698\n",
      "batch_loss:  0.9020059360450351\n",
      "batch_loss:  0.9010588208944461\n",
      "batch_loss:  0.8996118382646472\n",
      "batch_loss:  0.8986663845178036\n",
      "batch_loss:  0.901410117752659\n",
      "batch_loss:  0.898445401472249\n",
      "batch_loss:  0.9031380393184841\n",
      "batch_loss:  0.9025727603608228\n",
      "batch_loss:  0.901407608469344\n",
      "batch_loss:  0.8991479889538597\n",
      "batch_loss:  0.9018228494078764\n",
      "batch_loss:  0.9005503711683646\n",
      "batch_loss:  0.9047020135923391\n",
      "batch_loss:  0.8985594553292231\n",
      "batch_loss:  0.9022562656953095\n",
      "batch_loss:  0.9031262578431162\n",
      "batch_loss:  0.9015248689399158\n",
      "batch_loss:  0.901642493518683\n",
      "batch_loss:  0.8975194902638145\n",
      "batch_loss:  0.9013800596199556\n",
      "batch_loss:  0.9033573673860213\n",
      "batch_loss:  0.8999831134070297\n",
      "batch_loss:  0.9000859943579175\n",
      "batch_loss:  0.9028903264540966\n",
      "batch_loss:  0.9010433358259438\n",
      "batch_loss:  0.8992710159509543\n",
      "batch_loss:  0.9018336732650305\n",
      "batch_loss:  0.9011277584754817\n",
      "batch_loss:  0.9035446852427534\n",
      "batch_loss:  0.8997869732885786\n",
      "batch_loss:  0.901150061942907\n",
      "batch_loss:  0.8985040985388467\n",
      "batch_loss:  0.9023010067121361\n",
      "batch_loss:  0.9004713977375512\n",
      "batch_loss:  0.9006307699047666\n",
      "batch_loss:  0.899432056537049\n",
      "batch_loss:  0.9022359438828641\n",
      "batch_loss:  0.9017707663942097\n",
      "batch_loss:  0.9014781243931812\n",
      "batch_loss:  0.8998436046975452\n",
      "batch_loss:  0.9005239839272104\n",
      "batch_loss:  0.8984943597734906\n",
      "batch_loss:  0.9030490890057098\n",
      "batch_loss:  0.8996745787183659\n",
      "batch_loss:  0.8978676453617297\n",
      "batch_loss:  0.8990119416739094\n",
      "batch_loss:  0.9028864576115062\n",
      "batch_loss:  0.9025199674727932\n",
      "batch_loss:  0.8990656601310688\n",
      "batch_loss:  0.9008611256350453\n",
      "batch_loss:  0.9005435764732261\n",
      "batch_loss:  0.8991194088301836\n",
      "batch_loss:  0.9008598233135199\n",
      "batch_loss:  0.8978093556309164\n",
      "batch_loss:  0.9010471597471296\n",
      "batch_loss:  0.899329874385241\n",
      "batch_loss:  0.9001360647383198\n",
      "batch_loss:  0.9048223175202275\n",
      "batch_loss:  0.89981674364664\n",
      "batch_loss:  0.8977280497202049\n",
      "batch_loss:  0.9001431728184036\n",
      "batch_loss:  0.8963822782540839\n",
      "batch_loss:  0.898758512135592\n",
      "batch_loss:  0.9032414874941924\n",
      "batch_loss:  0.9020857465258174\n",
      "batch_loss:  0.9013837781549293\n",
      "batch_loss:  0.8991964461378893\n",
      "batch_loss:  0.9026241638640344\n",
      "batch_loss:  0.9007337449060182\n",
      "batch_loss:  0.9004887418635141\n",
      "batch_loss:  0.9001612484341291\n",
      "batch_loss:  0.9004364583769625\n",
      "batch_loss:  0.9004782883210588\n",
      "batch_loss:  0.8996964001296365\n",
      "batch_loss:  0.901652122781616\n",
      "batch_loss:  0.9020790175414406\n",
      "batch_loss:  0.9013782865492432\n",
      "batch_loss:  0.9006079296615475\n",
      "batch_loss:  0.8993592706342626\n",
      "batch_loss:  0.8987682383105118\n",
      "batch_loss:  0.9014025450598675\n",
      "batch_loss:  0.8996534312979471\n",
      "batch_loss:  0.9015574513213477\n",
      "batch_loss:  0.899794665471217\n",
      "batch_loss:  0.9023994483736247\n",
      "batch_loss:  0.8995033937534781\n",
      "batch_loss:  0.9010057257706502\n",
      "batch_loss:  0.9022295719685698\n",
      "batch_loss:  0.8995797851163836\n",
      "batch_loss:  0.9016200605759565\n",
      "batch_loss:  0.9007453808232277\n",
      "batch_loss:  0.899806890255701\n",
      "batch_loss:  0.9004192767163199\n",
      "batch_loss:  0.8991011813797225\n",
      "batch_loss:  0.9019811169574173\n",
      "batch_loss:  0.8998208928927501\n",
      "batch_loss:  0.9003619892533108\n",
      "batch_loss:  0.9032154726367135\n",
      "batch_loss:  0.8978587337134534\n",
      "batch_loss:  0.9012025855940441\n",
      "batch_loss:  0.903415649510951\n",
      "batch_loss:  0.8988745466038697\n",
      "batch_loss:  0.9013705431842706\n",
      "batch_loss:  0.9002963065025174\n",
      "batch_loss:  0.9002731381039314\n",
      "batch_loss:  0.9010149057614043\n",
      "batch_loss:  0.9011437287049295\n",
      "batch_loss:  0.9028249410685569\n",
      "batch_loss:  0.8990770729130155\n",
      "batch_loss:  0.9000461139110472\n",
      "batch_loss:  0.9017874466168982\n",
      "batch_loss:  0.900489589422408\n",
      "batch_loss:  0.9023854120214864\n",
      "batch_loss:  0.900598449935673\n",
      "batch_loss:  0.9018450227728327\n",
      "batch_loss:  0.8994891951744944\n",
      "batch_loss:  0.9003206099091188\n",
      "batch_loss:  0.9005891510433723\n",
      "batch_loss:  0.8995669184296353\n",
      "batch_loss:  0.9030853348292672\n",
      "batch_loss:  0.9021202256519056\n",
      "batch_loss:  0.8990573996668892\n",
      "batch_loss:  0.9009225340499235\n",
      "batch_loss:  0.9001638607510414\n",
      "batch_loss:  0.9006333110969728\n",
      "batch_loss:  0.9002327447972565\n",
      "batch_loss:  0.9008668785400613\n",
      "batch_loss:  0.9001245278786653\n",
      "batch_loss:  0.902106380793803\n",
      "batch_loss:  0.8993013175771649\n",
      "batch_loss:  0.9000865968226764\n",
      "batch_loss:  0.9014011837550858\n",
      "batch_loss:  0.8998839593456758\n",
      "batch_loss:  0.8996435256351458\n",
      "batch_loss:  0.9023472207972992\n",
      "batch_loss:  0.9013430875286983\n",
      "batch_loss:  0.8977065984162809\n",
      "batch_loss:  0.9011181645222264\n",
      "batch_loss:  0.902870698539173\n",
      "batch_loss:  0.8998208260057929\n",
      "batch_loss:  0.8965414717654522\n",
      "batch_loss:  0.9025614578292641\n",
      "batch_loss:  0.900769289562592\n",
      "batch_loss:  0.9018337617908582\n",
      "batch_loss:  0.9020017440105778\n",
      "batch_loss:  0.9043994333660521\n",
      "batch_loss:  0.8970537914666278\n",
      "batch_loss:  0.9006706484815902\n",
      "batch_loss:  0.9012784871074352\n",
      "batch_loss:  0.9005449311290576\n",
      "batch_loss:  0.9038659504839025\n",
      "batch_loss:  0.8992381344964433\n",
      "batch_loss:  0.9010974740294856\n",
      "batch_loss:  0.901662867129732\n",
      "batch_loss:  0.8991963653520348\n",
      "batch_loss:  0.9011438717027359\n",
      "batch_loss:  0.9000268730478544\n",
      "batch_loss:  0.901375517326581\n",
      "batch_loss:  0.8994415682039815\n",
      "batch_loss:  0.8984506138988461\n",
      "batch_loss:  0.8972902761888055\n",
      "batch_loss:  0.9072160157383539\n",
      "batch_loss:  0.8992577540685257\n",
      "batch_loss:  0.899976203824129\n",
      "batch_loss:  0.8979145178386462\n",
      "batch_loss:  0.9021838016881736\n",
      "batch_loss:  0.9033828908113075\n",
      "batch_loss:  0.9004715340055032\n",
      "batch_loss:  0.8994786128838699\n",
      "batch_loss:  0.9014545290625549\n",
      "batch_loss:  0.8981882206607746\n",
      "batch_loss:  0.8998885071828775\n",
      "batch_loss:  0.9022257994624688\n",
      "batch_loss:  0.9038521445559353\n",
      "batch_loss:  0.9012573829645163\n",
      "batch_loss:  0.9011458461372137\n",
      "batch_loss:  0.899608339721341\n",
      "batch_loss:  0.8987984301628594\n",
      "batch_loss:  0.9044415494556155\n",
      "batch_loss:  0.9029320163956404\n",
      "batch_loss:  0.9018779841230196\n",
      "batch_loss:  0.8991254188767691\n",
      "batch_loss:  0.903307150440915\n",
      "batch_loss:  0.8984429315671818\n",
      "batch_loss:  0.8984594015675779\n",
      "batch_loss:  0.9022264154968027\n",
      "batch_loss:  0.9005611762339538\n",
      "batch_loss:  0.9010455469680408\n",
      "batch_loss:  0.8989446249776951\n",
      "batch_loss:  0.8993141230778953\n",
      "batch_loss:  0.9008479157245156\n",
      "batch_loss:  0.9013539245582217\n",
      "batch_loss:  0.9021946091461476\n",
      "batch_loss:  0.8993865628864396\n",
      "batch_loss:  0.899500336621719\n",
      "batch_loss:  0.897169875246461\n",
      "batch_loss:  0.9056709227848878\n",
      "batch_loss:  0.9014738395599581\n",
      "batch_loss:  0.9027246539854342\n",
      "batch_loss:  0.9008838843495588\n",
      "batch_loss:  0.8985506647382009\n",
      "batch_loss:  0.8969911905134063\n",
      "batch_loss:  0.901319951626823\n",
      "batch_loss:  0.9014982886397488\n",
      "batch_loss:  0.9030950607690604\n",
      "batch_loss:  0.8984070517039987\n",
      "batch_loss:  0.9003392412512383\n",
      "batch_loss:  0.9028257585255257\n",
      "batch_loss:  0.9025459622072758\n",
      "batch_loss:  0.9012175859972846\n",
      "batch_loss:  0.9015550474303621\n",
      "batch_loss:  0.9018964685783206\n",
      "batch_loss:  0.8985856181263085\n",
      "batch_loss:  0.9014609808238988\n",
      "batch_loss:  0.9022643342424295\n",
      "batch_loss:  0.8992937842722755\n",
      "batch_loss:  0.8977248221157269\n",
      "batch_loss:  0.9014614311543157\n",
      "batch_loss:  0.8996524964381826\n",
      "batch_loss:  0.9024035267037407\n",
      "batch_loss:  0.9019816762816923\n",
      "batch_loss:  0.8987218122855636\n",
      "batch_loss:  0.8995410491625183\n",
      "batch_loss:  0.9023822635689407\n",
      "batch_loss:  0.8994400262972203\n",
      "batch_loss:  0.9011224302984614\n",
      "batch_loss:  0.9037264143044063\n",
      "batch_loss:  0.9005761804515356\n",
      "batch_loss:  0.9002347618485322\n",
      "batch_loss:  0.8998769060059958\n",
      "batch_loss:  0.9037497621850688\n",
      "batch_loss:  0.8997841810542754\n",
      "batch_loss:  0.9003011192377306\n",
      "batch_loss:  0.9017940230210395\n",
      "batch_loss:  0.9018324325812834\n",
      "batch_loss:  0.8982713743359971\n",
      "batch_loss:  0.9015141763108973\n",
      "batch_loss:  0.898344878828797\n",
      "batch_loss:  0.9028414018934354\n",
      "batch_loss:  0.904495130392919\n",
      "batch_loss:  0.8987323924388773\n",
      "batch_loss:  0.901434235545072\n",
      "batch_loss:  0.9016610629085986\n",
      "batch_loss:  0.899608217768031\n",
      "batch_loss:  0.9032227306400873\n",
      "batch_loss:  0.8999646555257678\n",
      "batch_loss:  0.9003839691663517\n",
      "batch_loss:  0.8992782370706776\n",
      "batch_loss:  0.9011438840778131\n",
      "batch_loss:  0.900865690738478\n",
      "batch_loss:  0.9012527835308971\n",
      "batch_loss:  0.9017235593329276\n",
      "batch_loss:  0.9000261413243514\n",
      "batch_loss:  0.9004420384370637\n",
      "batch_loss:  0.899600374321468\n",
      "batch_loss:  0.897550662998372\n",
      "batch_loss:  0.9027056260781846\n",
      "batch_loss:  0.9033596310829612\n",
      "batch_loss:  0.8996844451701059\n",
      "batch_loss:  0.9006809406189035\n",
      "batch_loss:  0.9035534842503847\n",
      "batch_loss:  0.899696964593068\n",
      "batch_loss:  0.9001804270364805\n",
      "batch_loss:  0.8997749838963162\n",
      "batch_loss:  0.9039104098904686\n",
      "batch_loss:  0.902000729032611\n",
      "batch_loss:  0.8998259019793725\n",
      "batch_loss:  0.8975242429398742\n",
      "batch_loss:  0.9034376161381138\n",
      "batch_loss:  0.8993127034699994\n",
      "batch_loss:  0.8998662545438405\n",
      "batch_loss:  0.9013916820169068\n",
      "batch_loss:  0.8973383074433912\n",
      "batch_loss:  0.9031018132431784\n",
      "batch_loss:  0.9004313466207471\n",
      "batch_loss:  0.8991392452953297\n",
      "batch_loss:  0.902559154041391\n",
      "batch_loss:  0.8998201025147877\n",
      "batch_loss:  0.9013367959567506\n",
      "batch_loss:  0.9010321547763179\n",
      "batch_loss:  0.8983523935962221\n",
      "batch_loss:  0.9036725604737136\n",
      "batch_loss:  0.8990679781889829\n",
      "batch_loss:  0.8994169366642558\n",
      "batch_loss:  0.8974525511632879\n",
      "batch_loss:  0.90012034013904\n",
      "batch_loss:  0.899562732013672\n",
      "batch_loss:  0.9054409830769944\n",
      "batch_loss:  0.9013530217932529\n",
      "batch_loss:  0.9012858862893135\n",
      "batch_loss:  0.899791566237738\n",
      "batch_loss:  0.900372194172591\n",
      "batch_loss:  0.9042916002367667\n",
      "batch_loss:  0.9019039347615879\n",
      "batch_loss:  0.9000309101952705\n",
      "batch_loss:  0.8995388046916052\n",
      "batch_loss:  0.8990735045691076\n",
      "batch_loss:  0.9015995255416404\n",
      "batch_loss:  0.8984859363944605\n",
      "batch_loss:  0.9014489716544211\n",
      "batch_loss:  0.896053621087902\n",
      "batch_loss:  0.9025901290079217\n",
      "batch_loss:  0.900604662112057\n",
      "batch_loss:  0.8969783322698974\n",
      "batch_loss:  0.9019130311879355\n",
      "batch_loss:  0.9048253542325224\n",
      "batch_loss:  0.8986264147579567\n",
      "batch_loss:  0.8993436477443087\n",
      "batch_loss:  0.9012670923744365\n",
      "batch_loss:  0.8998194499053459\n",
      "batch_loss:  0.8961915231664958\n",
      "batch_loss:  0.9064740986310992\n",
      "batch_loss:  0.9007839594617241\n",
      "batch_loss:  0.9016982624187955\n",
      "batch_loss:  0.899731441697409\n",
      "batch_loss:  0.9006183054268297\n",
      "batch_loss:  0.8988825469139146\n",
      "batch_loss:  0.9029303944332325\n",
      "batch_loss:  0.9034490415049039\n",
      "batch_loss:  0.9000785684271935\n",
      "batch_loss:  0.8997172838644067\n",
      "batch_loss:  0.9012558965474916\n",
      "batch_loss:  0.9003688822478431\n",
      "batch_loss:  0.8997011650958708\n",
      "batch_loss:  0.8996847558784721\n",
      "batch_loss:  0.9010138085946877\n",
      "batch_loss:  0.8994442895519001\n",
      "batch_loss:  0.9008548251849278\n",
      "batch_loss:  0.904352780394343\n",
      "batch_loss:  0.902560283411975\n",
      "batch_loss:  0.898534856295417\n",
      "batch_loss:  0.8981970966562389\n",
      "batch_loss:  0.9008179819947638\n",
      "batch_loss:  0.8997242136926802\n",
      "batch_loss:  0.9039761820823949\n",
      "batch_loss:  0.9040548368071355\n",
      "batch_loss:  0.900454958687882\n",
      "batch_loss:  0.9009167715378636\n",
      "batch_loss:  0.9001888397666307\n",
      "batch_loss:  0.9012579321187598\n",
      "batch_loss:  0.9011316114793465\n",
      "batch_loss:  0.8995790624383951\n",
      "batch_loss:  0.8993134488947934\n",
      "batch_loss:  0.8981549491952602\n",
      "batch_loss:  0.8939197480355381\n",
      "batch_loss:  0.9052759410628215\n",
      "batch_loss:  0.9004279532437113\n",
      "batch_loss:  0.9008793115966326\n",
      "batch_loss:  0.9026588516224793\n",
      "batch_loss:  0.9040660585786747\n",
      "batch_loss:  0.9011599730903168\n",
      "batch_loss:  0.9010176367338968\n",
      "batch_loss:  0.9017811686614385\n",
      "batch_loss:  0.9000819595162389\n",
      "batch_loss:  0.8999985759294467\n",
      "batch_loss:  0.8986033151242674\n",
      "batch_loss:  0.9026266004197017\n",
      "batch_loss:  0.8989552421962628\n",
      "batch_loss:  0.9016231569057567\n",
      "batch_loss:  0.8979066149295382\n",
      "batch_loss:  0.9050836150170016\n",
      "batch_loss:  0.8970286231878297\n",
      "batch_loss:  0.902078869255112\n",
      "batch_loss:  0.900964815589403\n",
      "batch_loss:  0.9011407532326169\n",
      "batch_loss:  0.9035421432187383\n",
      "batch_loss:  0.8990179554736737\n",
      "batch_loss:  0.9003217847071788\n",
      "batch_loss:  0.9002156522670728\n",
      "batch_loss:  0.9019090882766969\n",
      "batch_loss:  0.9014788316548149\n",
      "batch_loss:  0.8988090811969373\n",
      "batch_loss:  0.9005224057962826\n",
      "batch_loss:  0.8972493255714622\n",
      "batch_loss:  0.9030368814660641\n",
      "batch_loss:  0.8994767609569537\n",
      "batch_loss:  0.9047185635002392\n",
      "batch_loss:  0.9007764811099642\n",
      "batch_loss:  0.9017061963393315\n",
      "batch_loss:  0.9013003273821927\n",
      "batch_loss:  0.8993585565818631\n",
      "batch_loss:  0.9006680863795579\n",
      "batch_loss:  0.9020099109502014\n",
      "batch_loss:  0.8997931505836964\n",
      "batch_loss:  0.9004536817532184\n",
      "batch_loss:  0.9007014798070023\n",
      "batch_loss:  0.9023171605056447\n",
      "batch_loss:  0.9023497804145193\n",
      "batch_loss:  0.9004747194629167\n",
      "batch_loss:  0.9014786322777186\n",
      "batch_loss:  0.9013259294546044\n",
      "batch_loss:  0.9006961441534795\n",
      "batch_loss:  0.9002674542394784\n",
      "batch_loss:  0.8973723331128604\n",
      "batch_loss:  0.9022056203858557\n",
      "batch_loss:  0.9008813268094527\n",
      "batch_loss:  0.9002709058181406\n",
      "batch_loss:  0.9031679024446407\n",
      "batch_loss:  0.9019992614338622\n",
      "batch_loss:  0.9022782864534561\n",
      "batch_loss:  0.9021361226698156\n",
      "batch_loss:  0.9009407169829382\n",
      "batch_loss:  0.9003945711858471\n",
      "batch_loss:  0.9007675187219593\n",
      "batch_loss:  0.902208239709067\n",
      "batch_loss:  0.899270050702136\n",
      "batch_loss:  0.9011473582717532\n",
      "batch_loss:  0.9012861136856587\n",
      "batch_loss:  0.8994917435781946\n",
      "batch_loss:  0.9011302341471304\n",
      "batch_loss:  0.8984296037367422\n",
      "batch_loss:  0.9013500208287809\n",
      "batch_loss:  0.9015518520729897\n",
      "batch_loss:  0.898895565376815\n",
      "batch_loss:  0.9053808524776606\n",
      "batch_loss:  0.9007861338653259\n",
      "batch_loss:  0.9008489661613571\n",
      "batch_loss:  0.8999175015924118\n",
      "batch_loss:  0.9040596101536535\n",
      "batch_loss:  0.9002351745357392\n",
      "batch_loss:  0.899339141711994\n",
      "batch_loss:  0.9016099766600655\n",
      "batch_loss:  0.9010482102692137\n",
      "batch_loss:  0.9008635383559356\n",
      "batch_loss:  0.9031998312565793\n",
      "batch_loss:  0.901027922579895\n",
      "batch_loss:  0.9002724196164185\n",
      "batch_loss:  0.9015041447757377\n",
      "batch_loss:  0.9003285272604902\n",
      "batch_loss:  0.8979281820266657\n",
      "batch_loss:  0.9003089307924174\n",
      "batch_loss:  0.9023468039642656\n",
      "batch_loss:  0.8991673062926218\n",
      "batch_loss:  0.8993428174644762\n",
      "batch_loss:  0.9057279068069453\n",
      "batch_loss:  0.9010217818181843\n",
      "batch_loss:  0.9020706201007404\n",
      "batch_loss:  0.9022059800075978\n",
      "batch_loss:  0.8994127273181489\n",
      "batch_loss:  0.9032402769813623\n",
      "batch_loss:  0.900952157836457\n",
      "batch_loss:  0.901819418876736\n",
      "batch_loss:  0.9005622980034643\n",
      "batch_loss:  0.9031550576789944\n",
      "batch_loss:  0.899727013626901\n",
      "batch_loss:  0.8998248387138486\n",
      "batch_loss:  0.9007579741375351\n",
      "batch_loss:  0.9016792841118556\n",
      "batch_loss:  0.9002649927768975\n",
      "batch_loss:  0.8994190290505069\n",
      "batch_loss:  0.8986805128687029\n",
      "batch_loss:  0.9030800874758488\n",
      "batch_loss:  0.897747373330844\n",
      "batch_loss:  0.905012602316008\n",
      "batch_loss:  0.8993652133589275\n",
      "batch_loss:  0.9010936992691241\n",
      "batch_loss:  0.9007467790310963\n",
      "batch_loss:  0.8997278425795298\n",
      "batch_loss:  0.8979652713375649\n",
      "batch_loss:  0.9013431685994219\n",
      "batch_loss:  0.9031079044778053\n",
      "batch_loss:  0.8967143047598408\n",
      "batch_loss:  0.8982940308094384\n",
      "batch_loss:  0.9036117421662998\n",
      "batch_loss:  0.9009982156310861\n",
      "batch_loss:  0.9012747777016891\n",
      "batch_loss:  0.9002244580350248\n",
      "batch_loss:  0.8986906373906801\n",
      "batch_loss:  0.8996196343334316\n",
      "batch_loss:  0.9004531182783864\n",
      "batch_loss:  0.9014274512566558\n",
      "batch_loss:  0.9002766702196358\n",
      "batch_loss:  0.9000028260217622\n",
      "batch_loss:  0.9000789923495303\n",
      "batch_loss:  0.9002472567573472\n",
      "batch_loss:  0.9011314626971594\n",
      "batch_loss:  0.9006654447462294\n",
      "batch_loss:  0.8999890823709332\n",
      "batch_loss:  0.9000564794825795\n",
      "batch_loss:  0.8996184594305011\n",
      "batch_loss:  0.8995299504297775\n",
      "batch_loss:  0.901117848435796\n",
      "epoch  1 out of  2\n",
      "batch_loss:  0.899114183128924\n",
      "batch_loss:  0.89976519654126\n",
      "batch_loss:  0.8995215264159395\n",
      "batch_loss:  0.899686124923151\n",
      "batch_loss:  0.8991156719207758\n",
      "batch_loss:  0.899959558402946\n",
      "batch_loss:  0.8996764538796242\n",
      "batch_loss:  0.9053151675061116\n",
      "batch_loss:  0.9014249164914377\n",
      "batch_loss:  0.8981341369580567\n",
      "batch_loss:  0.8981421221952446\n",
      "batch_loss:  0.8995643257481931\n",
      "batch_loss:  0.9017252283942225\n",
      "batch_loss:  0.9028514802310993\n",
      "batch_loss:  0.9010896891972859\n",
      "batch_loss:  0.9018081693408927\n",
      "batch_loss:  0.8986211750592112\n",
      "batch_loss:  0.9032322336159189\n",
      "batch_loss:  0.8988790235893793\n",
      "batch_loss:  0.8997642004179052\n",
      "batch_loss:  0.9027010137740392\n",
      "batch_loss:  0.899890679664887\n",
      "batch_loss:  0.8991320104028685\n",
      "batch_loss:  0.9032954225616774\n",
      "batch_loss:  0.896508828528477\n",
      "batch_loss:  0.9023541818604629\n",
      "batch_loss:  0.9032025365380072\n",
      "batch_loss:  0.9003429619410934\n",
      "batch_loss:  0.896878707747967\n",
      "batch_loss:  0.9028872282142106\n",
      "batch_loss:  0.9057733637134716\n",
      "batch_loss:  0.899984318642739\n",
      "batch_loss:  0.9026191857561003\n",
      "batch_loss:  0.9016448050269951\n",
      "batch_loss:  0.901305131368155\n",
      "batch_loss:  0.902014171380975\n",
      "batch_loss:  0.9022000091788454\n",
      "batch_loss:  0.9003919972367015\n",
      "batch_loss:  0.9005081942679446\n",
      "batch_loss:  0.8990828370056283\n",
      "batch_loss:  0.8993915970509897\n",
      "batch_loss:  0.902592426216554\n",
      "batch_loss:  0.9019732743988322\n",
      "batch_loss:  0.8984154701399986\n",
      "batch_loss:  0.9042416230603976\n",
      "batch_loss:  0.8988349442526606\n",
      "batch_loss:  0.9038951501974064\n",
      "batch_loss:  0.8998351478484584\n",
      "batch_loss:  0.8986298970139593\n",
      "batch_loss:  0.9005530420730902\n",
      "batch_loss:  0.8999413024383803\n",
      "batch_loss:  0.9002500910329932\n",
      "batch_loss:  0.900843763080877\n",
      "batch_loss:  0.8995445217436178\n",
      "batch_loss:  0.9008728523542228\n",
      "batch_loss:  0.8984831623269776\n",
      "batch_loss:  0.8999557614666203\n",
      "batch_loss:  0.9015743841581951\n",
      "batch_loss:  0.9001837046927377\n",
      "batch_loss:  0.8976663047914154\n",
      "batch_loss:  0.8991306624684517\n",
      "batch_loss:  0.9012009548863464\n",
      "batch_loss:  0.9000264987006916\n",
      "batch_loss:  0.9015122223193732\n",
      "batch_loss:  0.9001521257180726\n",
      "batch_loss:  0.8994814255179665\n",
      "batch_loss:  0.9023771749162849\n",
      "batch_loss:  0.899637953612039\n",
      "batch_loss:  0.9011151842299486\n",
      "batch_loss:  0.9002255190129059\n",
      "batch_loss:  0.8996772908794486\n",
      "batch_loss:  0.9002120745257086\n",
      "batch_loss:  0.9018419393762991\n",
      "batch_loss:  0.9020609069446085\n",
      "batch_loss:  0.8987048026978571\n",
      "batch_loss:  0.9007201673950276\n",
      "batch_loss:  0.897969427103171\n",
      "batch_loss:  0.8988704720709122\n",
      "batch_loss:  0.8995390069688726\n",
      "batch_loss:  0.9022133186771453\n",
      "batch_loss:  0.9004001235408866\n",
      "batch_loss:  0.8974621638272984\n",
      "batch_loss:  0.9002564901241105\n",
      "batch_loss:  0.9006924781783537\n",
      "batch_loss:  0.9043849233521043\n",
      "batch_loss:  0.8972497791028842\n",
      "batch_loss:  0.9004220754470339\n",
      "batch_loss:  0.9038389016793446\n",
      "batch_loss:  0.9001933558783403\n",
      "batch_loss:  0.8986605030415692\n",
      "batch_loss:  0.9017843653218754\n",
      "batch_loss:  0.9014285867495705\n",
      "batch_loss:  0.8994604656714958\n",
      "batch_loss:  0.9057269982855314\n",
      "batch_loss:  0.9004511669047962\n",
      "batch_loss:  0.9006633902203853\n",
      "batch_loss:  0.8995020197235294\n",
      "batch_loss:  0.9029885065515141\n",
      "batch_loss:  0.9009404836285357\n",
      "batch_loss:  0.8995508164362578\n",
      "batch_loss:  0.8984606627902804\n",
      "batch_loss:  0.901731724753617\n",
      "batch_loss:  0.9003375067063817\n",
      "batch_loss:  0.8997836315125869\n",
      "batch_loss:  0.9023128881747857\n",
      "batch_loss:  0.900778336071631\n",
      "batch_loss:  0.9012297797683249\n",
      "batch_loss:  0.9037901624887568\n",
      "batch_loss:  0.8992412567802975\n",
      "batch_loss:  0.9004048089287244\n",
      "batch_loss:  0.9000651234913363\n",
      "batch_loss:  0.9022713527061466\n",
      "batch_loss:  0.8981211487153743\n",
      "batch_loss:  0.9019617703080081\n",
      "batch_loss:  0.901625826149045\n",
      "batch_loss:  0.9008451910366972\n",
      "batch_loss:  0.8986443899142923\n",
      "batch_loss:  0.9026656178383348\n",
      "batch_loss:  0.9019867582303486\n",
      "batch_loss:  0.9009908693190449\n",
      "batch_loss:  0.899584343588114\n",
      "batch_loss:  0.8986313462757688\n",
      "batch_loss:  0.9013515255257936\n",
      "batch_loss:  0.89845126022442\n",
      "batch_loss:  0.9029950852705744\n",
      "batch_loss:  0.9025196812157047\n",
      "batch_loss:  0.9013610382116086\n",
      "batch_loss:  0.8991058883108365\n",
      "batch_loss:  0.9018246640287727\n",
      "batch_loss:  0.9005022215712281\n",
      "batch_loss:  0.9046000720542012\n",
      "batch_loss:  0.8984573959135348\n",
      "batch_loss:  0.9021392669456975\n",
      "batch_loss:  0.9030323503568585\n",
      "batch_loss:  0.901456367038126\n",
      "batch_loss:  0.9015502579438407\n",
      "batch_loss:  0.8975564947104583\n",
      "batch_loss:  0.9012878343318631\n",
      "batch_loss:  0.9032725433681725\n",
      "batch_loss:  0.8999343555707743\n",
      "batch_loss:  0.9000573231390185\n",
      "batch_loss:  0.9028810725046751\n",
      "batch_loss:  0.9009339398706316\n",
      "batch_loss:  0.8992288056894147\n",
      "batch_loss:  0.9017852322098596\n",
      "batch_loss:  0.9010665870729394\n",
      "batch_loss:  0.9034459362089128\n",
      "batch_loss:  0.8997965344316263\n",
      "batch_loss:  0.9010033877538375\n",
      "batch_loss:  0.8985356822058371\n",
      "batch_loss:  0.9021705186405684\n",
      "batch_loss:  0.9004441668869039\n",
      "batch_loss:  0.9005153352560861\n",
      "batch_loss:  0.8994016401688388\n",
      "batch_loss:  0.9021539318456171\n",
      "batch_loss:  0.9016896281006965\n",
      "batch_loss:  0.9014651272496805\n",
      "batch_loss:  0.8998580583134731\n",
      "batch_loss:  0.9004793014496105\n",
      "batch_loss:  0.8984539076815774\n",
      "batch_loss:  0.9029608674151132\n",
      "batch_loss:  0.8997253454098937\n",
      "batch_loss:  0.8978368032763868\n",
      "batch_loss:  0.8990010109490266\n",
      "batch_loss:  0.9027744544765867\n",
      "batch_loss:  0.9024634754073443\n",
      "batch_loss:  0.8990146413646118\n",
      "batch_loss:  0.9008330327593214\n",
      "batch_loss:  0.9006135228469674\n",
      "batch_loss:  0.8990135335652084\n",
      "batch_loss:  0.9007797913560702\n",
      "batch_loss:  0.8978063972543027\n",
      "batch_loss:  0.9009993902990181\n",
      "batch_loss:  0.899331899279829\n",
      "batch_loss:  0.9000597053646203\n",
      "batch_loss:  0.9046984194185089\n",
      "batch_loss:  0.8997215309487469\n",
      "batch_loss:  0.8977272933729384\n",
      "batch_loss:  0.9000921864202267\n",
      "batch_loss:  0.8964111675276625\n",
      "batch_loss:  0.8987688535193822\n",
      "batch_loss:  0.9031978763173458\n",
      "batch_loss:  0.9019678257652768\n",
      "batch_loss:  0.9013614047877365\n",
      "batch_loss:  0.8992048122073332\n",
      "batch_loss:  0.9025174632427699\n",
      "batch_loss:  0.9006962337074407\n",
      "batch_loss:  0.9004804127604448\n",
      "batch_loss:  0.900124099297785\n",
      "batch_loss:  0.9003734681441086\n",
      "batch_loss:  0.900416689999582\n",
      "batch_loss:  0.8996897504139326\n",
      "batch_loss:  0.901553994225412\n",
      "batch_loss:  0.9020958093682907\n",
      "batch_loss:  0.9013175946214244\n",
      "batch_loss:  0.9005624355212626\n",
      "batch_loss:  0.8993461858667959\n",
      "batch_loss:  0.8988072780568608\n",
      "batch_loss:  0.9013649117091432\n",
      "batch_loss:  0.8996496757167424\n",
      "batch_loss:  0.9014821389406101\n",
      "batch_loss:  0.8997573257573872\n",
      "batch_loss:  0.9023140492567289\n",
      "batch_loss:  0.8994597721918425\n",
      "batch_loss:  0.9009977789606358\n",
      "batch_loss:  0.9022115633920927\n",
      "batch_loss:  0.8995132488960393\n",
      "batch_loss:  0.9015793935266398\n",
      "batch_loss:  0.9006407372666695\n",
      "batch_loss:  0.8998035240924118\n",
      "batch_loss:  0.9003833844332222\n",
      "batch_loss:  0.8991175899145112\n",
      "batch_loss:  0.9019400263052414\n",
      "batch_loss:  0.8997992701790827\n",
      "batch_loss:  0.9003567643739673\n",
      "batch_loss:  0.9030631780618944\n",
      "batch_loss:  0.8979347194606281\n",
      "batch_loss:  0.9011249902397425\n",
      "batch_loss:  0.9033368263516759\n",
      "batch_loss:  0.8988186810861033\n",
      "batch_loss:  0.9012728588836655\n",
      "batch_loss:  0.9002188726665387\n",
      "batch_loss:  0.9002683964890462\n",
      "batch_loss:  0.9009915399677598\n",
      "batch_loss:  0.9009759782283178\n",
      "batch_loss:  0.9028096818773876\n",
      "batch_loss:  0.8990337098501496\n",
      "batch_loss:  0.8999887759689785\n",
      "batch_loss:  0.9017464490899242\n",
      "batch_loss:  0.9004453500901523\n",
      "batch_loss:  0.9022916858227248\n",
      "batch_loss:  0.9005739226930689\n",
      "batch_loss:  0.9017848941466619\n",
      "batch_loss:  0.8994733020887266\n",
      "batch_loss:  0.9003484730728374\n",
      "batch_loss:  0.9005733084669315\n",
      "batch_loss:  0.8996104517195139\n",
      "batch_loss:  0.9029485403462141\n",
      "batch_loss:  0.9020474695853382\n",
      "batch_loss:  0.8990525551179458\n",
      "batch_loss:  0.9008924421189047\n",
      "batch_loss:  0.9001188326816119\n",
      "batch_loss:  0.9006009634031638\n",
      "batch_loss:  0.9002105505009773\n",
      "batch_loss:  0.9008388951104621\n",
      "batch_loss:  0.900112362869608\n",
      "batch_loss:  0.9020051473335028\n",
      "batch_loss:  0.8992764920923283\n",
      "batch_loss:  0.9000793970523696\n",
      "batch_loss:  0.9013290789077657\n",
      "batch_loss:  0.8998145416655623\n",
      "batch_loss:  0.8996254793051793\n",
      "batch_loss:  0.9022341410945603\n",
      "batch_loss:  0.9012889408568633\n",
      "batch_loss:  0.8977057182263981\n",
      "batch_loss:  0.901055460147023\n",
      "batch_loss:  0.9027983326467983\n",
      "batch_loss:  0.8996635199029709\n",
      "batch_loss:  0.8965243857175595\n",
      "batch_loss:  0.9024420612286199\n",
      "batch_loss:  0.9007545081177497\n",
      "batch_loss:  0.9017466939357017\n",
      "batch_loss:  0.9019721196839243\n",
      "batch_loss:  0.9044056043987178\n",
      "batch_loss:  0.8971076752408922\n",
      "batch_loss:  0.9005735880352119\n",
      "batch_loss:  0.9012840826762603\n",
      "batch_loss:  0.9004989949373844\n",
      "batch_loss:  0.9037718641147972\n",
      "batch_loss:  0.8991340825417444\n",
      "batch_loss:  0.9010376383442756\n",
      "batch_loss:  0.9016114743805629\n",
      "batch_loss:  0.8991710489160171\n",
      "batch_loss:  0.9011261109300903\n",
      "batch_loss:  0.9000371427386195\n",
      "batch_loss:  0.9012695956263924\n",
      "batch_loss:  0.8994447638696401\n",
      "batch_loss:  0.8984446329523291\n",
      "batch_loss:  0.8973495664066156\n",
      "batch_loss:  0.9070073678900917\n",
      "batch_loss:  0.8992583065018332\n",
      "batch_loss:  0.8999414076022965\n",
      "batch_loss:  0.8978982409581594\n",
      "batch_loss:  0.9020881743377273\n",
      "batch_loss:  0.9033749325295976\n",
      "batch_loss:  0.9004009839392902\n",
      "batch_loss:  0.899485927805383\n",
      "batch_loss:  0.9014477067481701\n",
      "batch_loss:  0.8981569527255827\n",
      "batch_loss:  0.8998673349782859\n",
      "batch_loss:  0.9021040990335851\n",
      "batch_loss:  0.9038016896767449\n",
      "batch_loss:  0.9011304797374886\n",
      "batch_loss:  0.9011422379067353\n",
      "batch_loss:  0.8995800414088393\n",
      "batch_loss:  0.898821695051272\n",
      "batch_loss:  0.904250213657898\n",
      "batch_loss:  0.9029242614638201\n",
      "batch_loss:  0.9018182778112973\n",
      "batch_loss:  0.8990850204763491\n",
      "batch_loss:  0.9032005179896985\n",
      "batch_loss:  0.8984019881616948\n",
      "batch_loss:  0.8984793815096771\n",
      "batch_loss:  0.9021092715486447\n",
      "batch_loss:  0.9005492396948191\n",
      "batch_loss:  0.901026556654458\n",
      "batch_loss:  0.8989089203439313\n",
      "batch_loss:  0.8993342344158165\n",
      "batch_loss:  0.9008552769681802\n",
      "batch_loss:  0.9011924966813014\n",
      "batch_loss:  0.9021284371923811\n",
      "batch_loss:  0.8993195525104619\n",
      "batch_loss:  0.899488195365383\n",
      "batch_loss:  0.8971768598933856\n",
      "batch_loss:  0.9055879691501427\n",
      "batch_loss:  0.9014122810877535\n",
      "batch_loss:  0.9026365057300229\n",
      "batch_loss:  0.9008402440910411\n",
      "batch_loss:  0.898465081524175\n",
      "batch_loss:  0.8970254002712883\n",
      "batch_loss:  0.9012611522525801\n",
      "batch_loss:  0.901442015071127\n",
      "batch_loss:  0.9030437761633529\n",
      "batch_loss:  0.8983758307169059\n",
      "batch_loss:  0.9002347445578941\n",
      "batch_loss:  0.902741224151028\n",
      "batch_loss:  0.902543839887392\n",
      "batch_loss:  0.9011498832729571\n",
      "batch_loss:  0.9014225666665108\n",
      "batch_loss:  0.9018629820191252\n",
      "batch_loss:  0.8986514474825559\n",
      "batch_loss:  0.9013652354817139\n",
      "batch_loss:  0.9022116016778425\n",
      "batch_loss:  0.8992816195384131\n",
      "batch_loss:  0.8976858548258285\n",
      "batch_loss:  0.9013813254567876\n",
      "batch_loss:  0.8995882557282516\n",
      "batch_loss:  0.9023188505663154\n",
      "batch_loss:  0.901969062057947\n",
      "batch_loss:  0.8987258676565496\n",
      "batch_loss:  0.8995168126748836\n",
      "batch_loss:  0.9022446879240192\n",
      "batch_loss:  0.89943915724314\n",
      "batch_loss:  0.9011068724799631\n",
      "batch_loss:  0.9036192319597258\n",
      "batch_loss:  0.9005276391649079\n",
      "batch_loss:  0.9001408267922002\n",
      "batch_loss:  0.899816511340775\n",
      "batch_loss:  0.9036921232098669\n",
      "batch_loss:  0.8997313453488016\n",
      "batch_loss:  0.90029965027306\n",
      "batch_loss:  0.9017039549201462\n",
      "batch_loss:  0.9017855426644289\n",
      "batch_loss:  0.8982292905080351\n",
      "batch_loss:  0.9014340771723893\n",
      "batch_loss:  0.8982776111686115\n",
      "batch_loss:  0.9027463294373368\n",
      "batch_loss:  0.9044201424008912\n",
      "batch_loss:  0.8986825880592905\n",
      "batch_loss:  0.9014005137438127\n",
      "batch_loss:  0.9015524289804903\n",
      "batch_loss:  0.8995802503495071\n",
      "batch_loss:  0.9031475402375387\n",
      "batch_loss:  0.8999955285132261\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 23\u001b[0m\n\u001b[1;32m      9\u001b[0m labels_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST/train-labels.idx1-ubyte\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m cfg \u001b[38;5;241m=\u001b[39m Config(input_layer_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m     12\u001b[0m               output_layer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     13\u001b[0m               hidden_layer_num\u001b[38;5;241m=\u001b[39m hidden_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m               epochs\u001b[38;5;241m=\u001b[39mepochs\n\u001b[1;32m     20\u001b[0m               )\n\u001b[0;32m---> 23\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m weights_layer_1 \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m weights_layer_2 \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[54], line 244\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(images_pth, labels_pth, cfg)\u001b[0m\n\u001b[1;32m    242\u001b[0m input_ly, label_ly \u001b[38;5;241m=\u001b[39m create_batch(cfg, index, f, l)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m#batch_learning(input_ly, label_ly, net,cfg)\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m W_delta, B_delta \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mhidden_layer_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):   \n\u001b[1;32m    246\u001b[0m     net\u001b[38;5;241m.\u001b[39mweights[layer_num] \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mweights[layer_num] \u001b[38;5;241m+\u001b[39m W_delta[layer_num]\n",
      "Cell \u001b[0;32mIn[54], line 204\u001b[0m, in \u001b[0;36mbatch_learning\u001b[0;34m(input_batch, desired_output_batch, net, cfg)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m    203\u001b[0m     net \u001b[38;5;241m=\u001b[39m forward(input_batch[batch], net)\n\u001b[0;32m--> 204\u001b[0m     w_del_batch_tmp, b_del_batch_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mbackprop_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_output_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mhidden_layer_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):   \n\u001b[1;32m    207\u001b[0m         w_del_batch[layer_num][batch,:,:] \u001b[38;5;241m=\u001b[39m w_del_batch_tmp[layer_num]\n",
      "Cell \u001b[0;32mIn[54], line 146\u001b[0m, in \u001b[0;36mbackprop_single\u001b[0;34m(net, expected_out_ly)\u001b[0m\n\u001b[1;32m    142\u001b[0m     b_delta[w] \u001b[38;5;241m=\u001b[39m a_error[w]\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     a_error[l] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_error\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sigmond_prime(net\u001b[38;5;241m.\u001b[39mz_layers[l])\n\u001b[1;32m    147\u001b[0m     w_delta[w] \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mouter(net\u001b[38;5;241m.\u001b[39mlayers[l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], a_error[l])\n\u001b[1;32m    148\u001b[0m     b_delta[w] \u001b[38;5;241m=\u001b[39m a_error[w]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_size = 28**2\n",
    "batch_size = 100\n",
    "dataset_size = 60000\n",
    "hidden_layers = 3\n",
    "hidden_layer_size = 17\n",
    "learning_rate = 1\n",
    "epochs = 2\n",
    "images_path = \"MNIST/train-images.idx3-ubyte\"\n",
    "labels_path = \"MNIST/train-labels.idx1-ubyte\"\n",
    "\n",
    "cfg = Config(input_layer_size=image_size,\n",
    "              output_layer_size=10,\n",
    "              hidden_layer_num= hidden_layers,\n",
    "              hidden_layer_size = hidden_layer_size,\n",
    "              \n",
    "              learning_rate=learning_rate,\n",
    "              dataset_size=dataset_size,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs\n",
    "              )\n",
    "\n",
    "\n",
    "net = train(images_path, labels_path, cfg=cfg)\n",
    "weights_layer_1 = net.weights[0]\n",
    "weights_layer_2 = net.weights[1]\n",
    "weights_layer_3 = net.weights[2]\n",
    "weights_layer_4 = net.weights[3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.09540138 0.10369053 0.10256694 0.10163326 0.09941897 0.08973942\n",
      " 0.10664583 0.10665082 0.09521669 0.09856967]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"MNIST/t10k-images.idx3-ubyte\", 'rb')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 50\n",
    "choice = 10\n",
    "\n",
    "try:\n",
    "    f.read(16)\n",
    "except UnicodeDecodeError as error:\n",
    "    print(error)\n",
    "    print(\"utf-8 error is usally due to binary format\")\n",
    "\n",
    "\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "batch = data.reshape(num_images, 28*28)\n",
    "data = data.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(data[choice]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "l = open(\"MNIST/t10k-labels.idx1-ubyte\",'rb')\n",
    "l.read(8)\n",
    "buf = l.read(10000)\n",
    "labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "print(labels[choice])\n",
    "\n",
    "inlayer = batch[choice]\n",
    "net = forward(inlayer, net)\n",
    "print(net.layers[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "net = create_network(3,5,1,2)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,1,0]\n",
    "\n",
    "net.weights[1] = np.transpose(np.array([[0,0],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1],\n",
    "                           [0.3,-1],\n",
    "                           [0.1,0.1]]))\n",
    "net.weights[0] = (np.array([[1,1],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1]]))\n",
    "print(np.shape(net.weights[1]))\n",
    "\n",
    "net = forward(input,net)\n",
    "w_delta, b_delta = backprop_single(net, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_network(3,5,2,7)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,1,0]\n",
    "\n",
    "net = forward(input,net)\n",
    "w_delta, b_delta = backprop_single(net, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
