{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create a nerual network without specialized libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "\n",
    "create layers,\n",
    "create weights and biases,\n",
    "create loss function,\n",
    "create forwards,\n",
    "    including squoosh\n",
    "create backprop,\n",
    "    create gradient descent!\n",
    "\n",
    "create all of the above without regard for crossing all training examples, handle that later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create layers and weights\n",
    "picture_size = 28 * 28\n",
    "input_size = picture_size\n",
    "output_size = 10\n",
    "fc_layer_size = 13\n",
    "\n",
    "inlayer = np.zeros(shape=(1,input_size), dtype=np.float32) \n",
    "layer1 = np.zeros(shape=(1,fc_layer_size), dtype=np.float32)\n",
    "layer2 = np.zeros(shape=(1,fc_layer_size), dtype=np.float32)\n",
    "layer3 = np.zeros(shape=(1,fc_layer_size), dtype=np.float32)\n",
    "outlayer = np.zeros(shape=(1,output_size), dtype=np.float32)\n",
    "\n",
    "layer1_wieghts = np.zeros(shape=(input_size,fc_layer_size), dtype=np.float32) #adding 1 for the bias\n",
    "layer2_wieghts = np.zeros(shape=(fc_layer_size,fc_layer_size), dtype=np.float32)\n",
    "layer3_wieghts = np.zeros(shape=(fc_layer_size,fc_layer_size), dtype=np.float32)\n",
    "outlayer_wieghts = np.zeros(shape=(fc_layer_size,output_size), dtype=np.float32)\n",
    "\n",
    "#inlayer = np.random.rand(1,input_size)\n",
    "layer1_wieghts = np.random.rand(input_size,fc_layer_size) #adding 1 for the bias\n",
    "layer2_wieghts = np.random.rand(fc_layer_size,fc_layer_size)\n",
    "layer3_wieghts = np.random.rand(fc_layer_size,fc_layer_size)\n",
    "outlayer_wieghts = np.random.rand(fc_layer_size,output_size)\n",
    "\n",
    "#inlayer[0,:] = [0,1,2,3,4]\n",
    "#layer1_wieghts[0,:] = [0,1,0,0.25,-1]\n",
    "#layer1_wieghts[:,0] = [0,1,0.5,0,-1]\n",
    "\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mathy bits\n",
    "import math\n",
    "\n",
    "def sigx(number):\n",
    "    try:\n",
    "        sig_x = ( 1 / (1+ math.exp(-number)))\n",
    "        #this does not woprk for large negitive numbers \n",
    "        # due to rounding errors leading to devide by zero so we'll add error handling\n",
    "    except OverflowError:\n",
    "        sig_x = 0\n",
    "    return(sig_x)\n",
    "\n",
    "def label_to_output(int):\n",
    "    outlayer = np.zeros(shape=(1,output_size), dtype=np.float32)\n",
    "    outlayer[0,int] = 1\n",
    "    return(outlayer)\n",
    "\n",
    "def single_layer_node_val(node_location, prior_layer, wieghts):\n",
    "    #n_v = sigx of weights and biases of previous layer\n",
    "    node_vl = 0\n",
    "    for index, node in enumerate(prior_layer[0,:]):\n",
    "        node_vl = node_vl + node * wieghts[index , node_location]\n",
    "    node_vl = sigx(node_vl/prior_layer.size)   \n",
    "    return(node_vl)\n",
    "\n",
    "def find_layer_node_vals(prior_layer, target_layer, weights):\n",
    "    tar_layer = np.zeros_like(target_layer)\n",
    "    for index, node in enumerate(tar_layer[0,:]):\n",
    "        tar_layer[0,index] = single_layer_node_val(index, prior_layer=prior_layer, wieghts=weights)\n",
    "        #print(\"index location\", index)\n",
    "    return tar_layer\n",
    "\n",
    "\n",
    "def find_loss_single(out_layer, expected_out):\n",
    "    node_loss = np.zeros(output_size, dtype=np.float32)\n",
    "    for index, value in enumerate(out_layer[0,:]):\n",
    "        node_loss[index] = (out_layer[0,index] - expected_out[0,index]) ** 2\n",
    "        #print(\"index:\", index)\n",
    "        #print(\"output value: \", out_layer[0,index])\n",
    "        #print(\"expected value: \", expected_out[0,index])\n",
    "        #print(\"node loss:\", index, node_loss[index])\n",
    "    MSE = np.sum(node_loss)/output_size\n",
    "    #RMSE = math.sqrt(MSE)\n",
    "    return(MSE)\n",
    "\n",
    "def backprop_single_layer(target_layer, prior_layer, desired_target_layer, weights):\n",
    "\n",
    "    current_A_delta = desired_target_layer - target_layer\n",
    "    A_delta = np.zeros_like(weights)\n",
    "    W_delta = np.zeros_like(weights)\n",
    "\n",
    "    for j, activation in enumerate(target_layer[0,:]):\n",
    "        for i, weight in enumerate(weights[j,:]):\n",
    "            W_delta[j, i] = current_A_delta[0,j] * prior_layer[0,i] * learning_rate\n",
    "            A_delta[j,i]  = current_A_delta[0,j] * weights[j,i] * learning_rate\n",
    "    \n",
    "    desired_prior_layer = np.zeros_like(prior_layer)\n",
    "    for i, neuron in enumerate(desired_prior_layer):\n",
    "        desired_prior_layer[0,i] = np.mean(A_delta[:,i])\n",
    "\n",
    "    return(W_delta, desired_prior_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_to_output(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwards(input_layer, layer1_wieghts, layer2_wieghts, layer3_wieghts, outlayer_wieghts):\n",
    "    layer1[0,:] = find_layer_node_vals(input_layer, layer1, layer1_wieghts)\n",
    "    layer2[0,:] = find_layer_node_vals(layer1, layer2, layer2_wieghts)\n",
    "    layer3[0,:] = find_layer_node_vals(layer2, layer3, layer3_wieghts)\n",
    "    outlayer[0,:] = find_layer_node_vals(layer3, outlayer, outlayer_wieghts)\n",
    "    return(outlayer)\n",
    "\n",
    "# back propigate\n",
    "def backprop(outlayer, outlayer_key):\n",
    "    w_delta_out, desired_prior_layer3 = backprop_single_layer(outlayer, layer3, outlayer_key, outlayer_wieghts)\n",
    "    w_delta_3, desired_prior_layer2 = backprop_single_layer(layer3, layer2, desired_prior_layer3, layer3_wieghts) # not sure if I should backprop the weight changes somehow too?\n",
    "    w_delta_2, desired_prior_layer1 = backprop_single_layer(layer2, layer1, desired_prior_layer2, layer2_wieghts)\n",
    "    w_delta_1, desired_prior_layer0 = backprop_single_layer(layer1, inlayer, desired_prior_layer1, layer1_wieghts)\n",
    "    w_deltas = [w_delta_1, w_delta_2, w_delta_3, w_delta_out]\n",
    "    return(w_deltas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image_size = 28\n",
    "batch_size = 100\n",
    "num_train_images = 60000\n",
    "\n",
    "num_batchs = num_train_images/batch_size\n",
    "epochs = 2\n",
    "loss_over_time = np.zeros([int(num_batchs)*epochs])\n",
    "\n",
    "\n",
    "for epoch in range(0,epochs,1):\n",
    "    print(\"epoch \", int(epoch), \"out of \", int(epochs))\n",
    "    f = open(\"MNIST/train-images.idx3-ubyte\", 'rb')\n",
    "    f.read(16)\n",
    "    l = open(\"MNIST/train-labels.idx1-ubyte\",'rb')\n",
    "    l.read(8)\n",
    "\n",
    "    for i in range(0,num_train_images,batch_size):\n",
    "        print(\"batch \", int(i/batch_size), \"out of \", int(num_batchs))\n",
    "        image_buffer = f.read(image_size * image_size * batch_size)\n",
    "        batch = np.frombuffer(image_buffer, dtype=np.uint8).astype(np.float32)\n",
    "        batch = batch.reshape(batch_size, (image_size**2))\n",
    "\n",
    "        label_buffer = l.read(batch_size)\n",
    "        label_batch = np.frombuffer(label_buffer, dtype=np.uint8).astype(np.int64)\n",
    "\n",
    "\n",
    "        # batch is a batch_size by 784 float32 array\n",
    "        w_1_d = np.zeros((batch_size, input_size, fc_layer_size))\n",
    "        w_2_d = np.zeros((batch_size, fc_layer_size, fc_layer_size))\n",
    "        w_3_d = np.zeros((batch_size, fc_layer_size, fc_layer_size))\n",
    "        w_out_d = np.zeros((batch_size, fc_layer_size, output_size))\n",
    "        loss_arr = np.zeros(batch_size)\n",
    "\n",
    "        for index, image in enumerate(batch):\n",
    "            inlayer[0,:] = image/255\n",
    "            outlayer = forwards(inlayer, layer1_wieghts, layer2_wieghts, layer3_wieghts, outlayer_wieghts)\n",
    "            outlayer_key = label_to_output(label_batch[index])\n",
    "            w_1_d[index,:], w_2_d[index,:], w_3_d[index,:], w_out_d[index,:] = backprop(outlayer, outlayer_key)\n",
    "            loss_arr[index] = find_loss_single(outlayer, outlayer_key)\n",
    "            \n",
    "\n",
    "        layer1_wieghts = layer1_wieghts + np.mean(w_1_d, axis=(0))\n",
    "        layer2_wieghts = layer2_wieghts + np.mean(w_2_d, axis=(0))\n",
    "        layer3_wieghts = layer3_wieghts + np.mean(w_3_d, axis=(0))\n",
    "        outlayer_wieghts = outlayer_wieghts + np.mean(w_out_d, axis=(0))\n",
    "        loss = np.mean(loss_arr)\n",
    "        print(loss)\n",
    "        loss_over_time[int((i/batch_size))*(epoch+1)] = loss\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "batch_number = np.arange(1, int(num_batchs*epochs + 1))\n",
    "\n",
    "\n",
    "plt.plot(batch_number,loss_over_time)\n",
    "\n",
    "plt.xlabel(\"batch_number\")\n",
    "plt.ylabel(\"loss_over_time\")\n",
    "plt.title(\"loss over batch number\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = open(\"MNIST/t10k-images.idx3-ubyte\", 'rb')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 50\n",
    "choice = 15\n",
    "\n",
    "try:\n",
    "    f.read(16)\n",
    "except UnicodeDecodeError as error:\n",
    "    print(error)\n",
    "    print(\"utf-8 error is usally due to binary format\")\n",
    "\n",
    "\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "batch = data.reshape(num_images, picture_size)\n",
    "data = data.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(data[choice]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "l = open(\"MNIST/t10k-labels.idx1-ubyte\",'rb')\n",
    "l.read(8)\n",
    "buf = l.read(10000)\n",
    "labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "print(labels[choice])\n",
    "\n",
    "inlayer[0,:] = batch[choice]\n",
    "outlayer = forwards(inlayer, layer1_wieghts, layer2_wieghts, layer3_wieghts, outlayer_wieghts)\n",
    "print(outlayer)\n",
    "\n",
    "print(\"loss\", find_loss_single(outlayer, label_to_output(labels[choice])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
