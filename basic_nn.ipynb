{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simaliar to the first attempt but with cleaner code, more flexible layer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "print(\"start!\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    input_layer_size: int\n",
    "    output_layer_size: int\n",
    "    hidden_layer_num : int\n",
    "    hidden_layer_size : int\n",
    "\n",
    "    dataset_size : int\n",
    "    learning_rate :float\n",
    "    batch_size : int\n",
    "    epochs : int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer(layer_size, dtype=np.float32, **kwargs):\n",
    "    layer = np.zeros(layer_size, dtype=dtype)\n",
    "    return(layer)\n",
    "\n",
    "def create_layer_weights(prev_ly_size, curr_ly_size, init=True):\n",
    "    if init:\n",
    "        #ly_weights = np.random.rand(prev_ly_size, curr_ly_size)\n",
    "        ly_weights = np.random.uniform(-0.5,0.5, size=(prev_ly_size, curr_ly_size))\n",
    "    else:\n",
    "        ly_weights = np.zeros(shape=(prev_ly_size, curr_ly_size))\n",
    "    return(ly_weights)\n",
    "\n",
    "def create_biases(ly_size, init=True):\n",
    "    if init:\n",
    "        biases = np.random.rand(ly_size)\n",
    "    else:\n",
    "        biases = np.zeros(ly_size)\n",
    "    return(biases)\n",
    "\n",
    "\n",
    "def label_to_output(int) -> np.ndarray:\n",
    "    outlayer = np.zeros(shape=(10), dtype=np.float32)\n",
    "    outlayer[int] = 1\n",
    "    return(outlayer)\n",
    "\n",
    "def label_vec_to_output(inputs: np.ndarray) -> np.ndarray:\n",
    "    # Create an identity matrix of size 10 and index into it\n",
    "    return np.eye(10, dtype=np.int8)[inputs]\n",
    "\n",
    "class n_network:\n",
    "    def __init__(self, layers, z_layers, biases, weights):\n",
    "        self.layers = layers\n",
    "        self.z_layers = z_layers\n",
    "        self.biases = biases\n",
    "        self.weights = weights\n",
    "\n",
    "def create_network(in_ly_size, out_ly_size, hidden_ly_num, hidden_ly_size):\n",
    "    in_layer = create_layer(in_ly_size)\n",
    "    hidden_lys = []\n",
    "    for index in range(hidden_ly_num):\n",
    "        hidden_lys.append(create_layer(hidden_ly_size))\n",
    "    out_layer = create_layer(out_ly_size)\n",
    "    network_lys = [in_layer, *hidden_lys, out_layer]\n",
    "\n",
    "\n",
    "    biases =[]\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        biases.append(create_biases(np.size(network_lys[i]), init=False))\n",
    "\n",
    "    weights = []\n",
    "    for i in range(1, len(network_lys), 1):\n",
    "        weights.append( create_layer_weights(np.size(network_lys[i-1]), np.size(network_lys[i]), init=True))\n",
    "\n",
    "    z_layers = deep_copy_mat_list(network_lys, False)\n",
    "\n",
    "    network = n_network(network_lys, z_layers, biases, weights)\n",
    "    return(network)\n",
    "\n",
    "\n",
    "def sigx(number):\n",
    "    #sigmond function\n",
    "    #todo: make a nativly vectorized verison of this \n",
    "    try:\n",
    "        sig_x = ( 1 / (1+ math.exp(-number)))\n",
    "        #this does not woprk for large negitive numbers \n",
    "        # due to rounding errors leading to devide by zero so we'll add error handling\n",
    "    except OverflowError:\n",
    "        sig_x = 0\n",
    "    return(sig_x)\n",
    "\n",
    "def sigmond(input):\n",
    "    #a more easily vectorizable sigmond function\n",
    "    return( 1/(1+np.exp(-input)))\n",
    "\n",
    "def sigmond_prime(input):\n",
    "\n",
    "    return(sigmond(input) * (1-sigmond(input)))\n",
    "\n",
    "def forward(input_vals:np.ndarray, net:n_network):\n",
    "    #first layer\n",
    "    input_vals = np.array(input_vals)\n",
    "    net.layers[0] = input_vals\n",
    "\n",
    "    for index in range(1, len(net.layers), 1):\n",
    "        net.layers[index] = np.dot(net.layers[(index-1)], net.weights[index-1]) + net.biases[index-1]\n",
    "        \n",
    "        #activation function\n",
    "        net.layers[index] = sigmond(net.layers[index])\n",
    "    return(net)\n",
    "\n",
    "def node_delta(expected:np.array, actual:np.array):\n",
    "    delta = expected - actual\n",
    "    return(delta)\n",
    "\n",
    "def deep_copy_mat_list (list_to_copy, propigate_cell_vals=True):\n",
    "    #creates a copy of a list containing numpy arrays of varible size, ethier as empty arrays or with the same values\n",
    "    \n",
    "    new_list = []\n",
    "    if propigate_cell_vals:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(items)\n",
    "    else:\n",
    "        for items in list_to_copy:\n",
    "            new_list.append(np.zeros_like(items))       \n",
    "\n",
    "    return(new_list)\n",
    "\n",
    "\n",
    "def loss(network_output_ly, expected_ly):\n",
    "    #loss =  mean of all output neurons - the expected, squared\n",
    "    loss_vec = ((network_output_ly - expected_ly)**2) / 2\n",
    "    loss_sclr = np.sum((network_output_ly - expected_ly)**2)\n",
    "    return(loss_sclr, loss_vec)\n",
    "\n",
    "def backprop_single(net: n_network, expected_out_ly):\n",
    "    #does backprop for a single training example, duhh\n",
    "    #add learning rate??\n",
    "    \n",
    "    expected_out_ly = np.array(expected_out_ly)\n",
    "    w_delta = deep_copy_mat_list(net.weights, propigate_cell_vals=False)\n",
    "    b_delta = deep_copy_mat_list(net.biases, propigate_cell_vals=False)\n",
    "    a_error = deep_copy_mat_list(net.layers, propigate_cell_vals=False)\n",
    "\n",
    "    first = True\n",
    "    for l in range(len(net.layers)-1, 0, -1):\n",
    "        w = l - 1\n",
    "\n",
    "        if first:\n",
    "            #first(last) layer\n",
    "            first = False\n",
    "            a_delta_dir = expected_out_ly - net.layers[l]\n",
    "            #print(\"a_delta_dir: \", len(a_delta_dir))\n",
    "            a_error[l] = a_delta_dir * sigmond_prime(net.z_layers[l])\n",
    "\n",
    "            a_error[l-1] = np.dot((net.weights[w]) , a_error[l]) * sigmond_prime(net.z_layers[l-1])\n",
    "\n",
    "            w_delta[w] =  np.outer(net.layers[l-1], a_error[l])\n",
    "            #print(\"w_delta of w: \", np.shape(w_delta[w]), np.shape(net.weights[w]))\n",
    "            b_delta[w] = a_error[l]\n",
    "\n",
    "        else:\n",
    "\n",
    "            a_error[l] = np.dot((net.weights[w+1]) , a_error[l+1]) * sigmond_prime(net.z_layers[l])\n",
    "            w_delta[w] =  np.outer(net.layers[l-1], a_error[l])\n",
    "            b_delta[w] = a_error[l]\n",
    "            #print(\"shape of b_delta: \", np.shape(b_delta[w]))\n",
    "\n",
    "    return(w_delta, b_delta)\n",
    "\n",
    "def backprop_single_OLD(net: n_network, expected_out_ly):\n",
    "    #does backprop for a single training example, duhh\n",
    "    #add learning rate??\n",
    "    \n",
    "    expected_out_ly = np.array(expected_out_ly)\n",
    "    w_delta = deep_copy_mat_list(net.weights, propigate_cell_vals=False)\n",
    "    b_delta = deep_copy_mat_list(net.biases, propigate_cell_vals=False)\n",
    "    a_delta = deep_copy_mat_list(net.layers, propigate_cell_vals=False)\n",
    "\n",
    "    first = True\n",
    "    for l in range(len(net.layers)-1, 0, -1):\n",
    "        #print(\"at l of:\", l)\n",
    "\n",
    "        if first:\n",
    "            #first(last) layer\n",
    "            first = False\n",
    "            a_delta_dir = expected_out_ly - net.layers[l]  \n",
    "            w_delta[l-1] = np.outer(net.layers[l-1], a_delta_dir)   \n",
    "            a_delta[l] = a_delta_dir\n",
    "        else:\n",
    "            a_delta_dir = a_delta[l] - net.layers[l]\n",
    "            w_delta[l-1] = np.outer(net.layers[l-1], a_delta_dir)   \n",
    "\n",
    "        a_delta[l-1] = np.dot(net.weights[l-1], net.layers[l])\n",
    "        a_delta[l-1] = a_delta[l-1] / a_delta[l-1].size\n",
    "    return(w_delta, b_delta)\n",
    "\n",
    "def create_batch(cfg:Config, batch_position, image_file, label_file):\n",
    "   \n",
    "    image_buffer = image_file.read(cfg.input_layer_size * cfg.batch_size)\n",
    "    input_batch = np.frombuffer(image_buffer, dtype=np.uint8).astype(np.float32)\n",
    "    input_batch = input_batch.reshape(cfg.batch_size, cfg.input_layer_size)\n",
    "    \n",
    "    label_buffer = label_file.read(cfg.batch_size)\n",
    "    label_batch = np.frombuffer(label_buffer, dtype=np.uint8).astype(np.int64)\n",
    "    Label_batch = label_vec_to_output(label_batch)\n",
    "\n",
    "    return(input_batch, Label_batch)\n",
    "    \n",
    "def batch_learning(input_batch:np.ndarray, desired_output_batch:np.ndarray, net:n_network, cfg:Config):\n",
    "    cfg.batch_size\n",
    "    w_del_batch = []\n",
    "    b_del_batch = []\n",
    "    batch_loss = np.ndarray((cfg.batch_size,))\n",
    "\n",
    "    for layers in net.weights:\n",
    "        w_del_batch.append(np.zeros((cfg.batch_size,) + layers.shape, dtype=np.float32)  )\n",
    "    for layers in net.biases:\n",
    "        b_del_batch.append(np.zeros((cfg.batch_size,) + layers.shape, dtype=np.float32)  )\n",
    "\n",
    "\n",
    "    for batch in range(cfg.batch_size):\n",
    "        net = forward(input_batch[batch], net)\n",
    "        w_del_batch_tmp, b_del_batch_tmp = backprop_single(net, desired_output_batch[batch])\n",
    "\n",
    "        for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "            w_del_batch[layer_num][batch,:,:] = w_del_batch_tmp[layer_num]\n",
    "            b_del_batch[layer_num][batch,:] = b_del_batch_tmp[layer_num]\n",
    "        \n",
    "        loss_sclr, loss_vec = loss(net.layers[-1], desired_output_batch[batch])\n",
    "        batch_loss[batch] = loss_sclr\n",
    "    batch_loss = np.mean(batch_loss)\n",
    "    print(\"batch_loss: \",batch_loss)\n",
    "\n",
    "\n",
    "\n",
    "    w_delta_mean = deep_copy_mat_list(net.weights, False)\n",
    "    b_delta_mean = deep_copy_mat_list(net.biases, False)\n",
    "    for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "        w_delta_mean[layer_num] = np.mean(w_del_batch[layer_num], axis=0) * cfg.learning_rate #learning rate is here!\n",
    "        b_delta_mean[layer_num] = np.mean(b_del_batch[layer_num], axis=0) * cfg.learning_rate\n",
    "    #    print(w_delta_mean[layer_num].shape)\n",
    "    #print(\"W_delta array: \\n\")\n",
    "    #for layers in w_delta_mean:\n",
    "    #    print(layers.shape)\n",
    "    #    print(layers)\n",
    "\n",
    "    descent_step = w_delta_mean, b_delta_mean\n",
    "    return(descent_step)\n",
    "\n",
    "def train(images_pth, labels_pth, cfg:Config):\n",
    "\n",
    "    net = create_network(cfg.input_layer_size, cfg.output_layer_size, cfg.hidden_layer_num, cfg.hidden_layer_size)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(\"epoch \", int(epoch), \"out of \", int(cfg.epochs))\n",
    "        f = open(images_pth, 'rb')\n",
    "        f.read(16)\n",
    "        l = open(labels_pth,'rb')\n",
    "        l.read(8)\n",
    "        for index in range(0, cfg.dataset_size, cfg.batch_size):\n",
    "            #print(\"index position: \", int(index))\n",
    "            input_ly, label_ly = create_batch(cfg, index, f, l)\n",
    "            #batch_learning(input_ly, label_ly, net,cfg)\n",
    "            W_delta, B_delta = batch_learning(input_ly, label_ly, net,cfg)\n",
    "            for layer_num in range(cfg.hidden_layer_num +1):   \n",
    "                net.weights[layer_num] = net.weights[layer_num] + W_delta[layer_num]\n",
    "                net.biases[layer_num] = net.biases[layer_num] + B_delta[layer_num]\n",
    "\n",
    "\n",
    "\n",
    "    #do batches here:\n",
    "    return(net)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28**2\n",
    "batch_size = 100\n",
    "dataset_size = 60000\n",
    "hidden_layers = 3\n",
    "hidden_layer_size = 17\n",
    "learning_rate = 0.05\n",
    "epochs = 10\n",
    "images_path = \"MNIST/train-images.idx3-ubyte\"\n",
    "labels_path = \"MNIST/train-labels.idx1-ubyte\"\n",
    "\n",
    "cfg = Config(input_layer_size=image_size,\n",
    "              output_layer_size=10,\n",
    "              hidden_layer_num= hidden_layers,\n",
    "              hidden_layer_size = hidden_layer_size,\n",
    "              \n",
    "              learning_rate=learning_rate,\n",
    "              dataset_size=dataset_size,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs\n",
    "              )\n",
    "\n",
    "\n",
    "net = train(images_path, labels_path, cfg=cfg)\n",
    "\n",
    "print\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"MNIST/t10k-images.idx3-ubyte\", 'rb')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 50\n",
    "choice = 10\n",
    "\n",
    "try:\n",
    "    f.read(16)\n",
    "except UnicodeDecodeError as error:\n",
    "    print(error)\n",
    "    print(\"utf-8 error is usally due to binary format\")\n",
    "\n",
    "\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "batch = data.reshape(num_images, 28*28)\n",
    "data = data.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(data[choice]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "l = open(\"MNIST/t10k-labels.idx1-ubyte\",'rb')\n",
    "l.read(8)\n",
    "buf = l.read(10000)\n",
    "labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "print(labels[choice])\n",
    "\n",
    "inlayer = batch[choice]\n",
    "net = forward(inlayer, net)\n",
    "print(net.layers[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_network(3,5,1,2)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,1,0]\n",
    "\n",
    "net.weights[1] = np.transpose(np.array([[0,0],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1],\n",
    "                           [0.3,-1],\n",
    "                           [0.1,0.1]]))\n",
    "net.weights[0] = (np.array([[1,1],\n",
    "                           [0.5,0.5],\n",
    "                           [0.3,-1]]))\n",
    "print(np.shape(net.weights[1]))\n",
    "\n",
    "net = forward(input,net)\n",
    "w_delta, b_delta = backprop_single(net, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_network(3,5,2,2)\n",
    "input = [1,1,1]\n",
    "output = [0,1,0,0,0]\n",
    "\n",
    "net = forward(input,net)\n",
    "w_delta, b_delta = backprop_single(net, output)\n",
    "\n",
    "for index in range(len(net.layers)):\n",
    "    print(\"index position: \", index)\n",
    "\n",
    "    print(\"layer size and shape: \", net.layers[index].shape)\n",
    "\n",
    "    try:\n",
    "        print(\"weights size and shape: \", net.weights[index].shape)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        print(\"bias size and shape: \", net.biases[index].shape)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(net.weights[-1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
